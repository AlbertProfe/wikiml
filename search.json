[
  {
    "objectID": "rags/rags-index.html",
    "href": "rags/rags-index.html",
    "title": "RAGs",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n\n\n Back to top",
    "crumbs": [
      "Retrieval-Augmented Generation",
      "RAGs"
    ]
  },
  {
    "objectID": "ann/ann-index.html",
    "href": "ann/ann-index.html",
    "title": "ANN: Table of Contents",
    "section": "",
    "text": "Artificial Neural Networks (ANNs) are computational models inspired by biological neural networks in human brains. They are designed to process complex data and perform tasks like classification, prediction, and pattern recognition.\n\n\n\n\n\n\n\n\n\nLayers:\n\nInput Layer: Initial point of data entry\nHidden Layers: Process and analyze data\nOutput Layer: Provides final computational results\n\n\n\n\n\n\n\n\n\nFeedforward Neural Networks (FNNs)\n\nData moves in one direction\nNo cycles or loops\nStraightforward processing\n\nConvolutional Neural Networks (CNNs)\n\nSpecialized for processing grid data (images)\nUses convolutional layers\nEffective for image recognition\n\nRecurrent Neural Networks (RNNs)\n\nDesigned for sequence prediction\nMaintains context across inputs\nUseful for time-series and language processing\n\n\n\n\nLearning Approaches:\n\nSupervised Learning: Uses labeled training data\nUnsupervised Learning: Discovers patterns in unlabeled data\nReinforcement Learning: Learns through environmental interaction\n\nTraining Process:\n\nForward Propagation\nLoss Function Calculation\nBackward Propagation (Backpropagation)\n\n\n\n\n\nStructure of Biological Neurons\n\n\n\n\nKey Areas:\n\nNatural Language Processing\nHealthcare\neCommerce\nComputer Vision\n\n\n\n\nLimitations:\n\nOverfitting\nLack of Interpretability\nHigh Training Data Requirements\nComputational Intensity\n\nEthical Considerations:\n\nBias in decision-making\nAccountability\nTransparency in automated systems",
    "crumbs": [
      "What is ANN?",
      "ANN: Table of Contents"
    ]
  },
  {
    "objectID": "bigdata/bigdata-index.html",
    "href": "bigdata/bigdata-index.html",
    "title": "Big Data",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n\n\n Back to top",
    "crumbs": [
      "What is Big Data?",
      "Big Data"
    ]
  },
  {
    "objectID": "python/python-index.html",
    "href": "python/python-index.html",
    "title": "Python",
    "section": "",
    "text": "Python Programming MOOC 2024\n\n\n\n\nThis is the course material page for the Introduction to Programming course (BSCS1001, 5 ECTS) and the Advanced Course in Programming (BSCS1002, 5 ECTS) from the Department of Computer Science at the University of Helsinki.\n\n\n\nThere are no more live lectures for this year’s MOOC instance. The lecture recordings can be viewed on the table below.\n\nLectures\n\n\n\n\n\nPython Programming MOOC 2024\nAll exercises\nPart 1: Getting started",
    "crumbs": [
      "Python Programming",
      "Python"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Wiki Machine Learning",
    "section": "",
    "text": "Machine learning (ML) is a branch of artificial intelligence (AI) focused on enabling computers and machines to imitate the way that humans learn, to perform tasks autonomously, and to improve their performance and accuracy through experience and exposure to more data.\n\nTransformers are all you need\n\nTransformers (how LLMs work) explained visually | DL5 by 3Blue1Brown\n\nA transformer, functioning as a perceptron within a multilayer large language model (LLM), utilizes self-attention to process input sequences.\nEach word in the sentence “The best Costa Brava spot is …” can attend to all other words, allowing the model to capture contextual relationships and dependencies effectively.\nThis is accomplished through multi-head attention, where multiple attention mechanisms operate in parallel, enabling the model to analyze different aspects of the input simultaneously.\nThe repetition of these processes across layers enhances the model’s ability to refine its predictions, ultimately generating coherent and contextually relevant completions for the sentence.\n\n\n\nMathematical Model of an ANN\nEach neuron computes a weighted sum of its inputs, adds a bias term, and applies an activation function. The output of a neuron can be represented as:\n\\[\ny = f\\left( \\sum_{i=1}^{n} w_i x_i + b \\right)\n\\]\nwhere ( w_i ) are the weights, ( x_i ) are the inputs, ( b ) is the bias, and ( f ) is the activation function.\n\n\n\nFrom: Optical neural networks: progress and challenges\n\n\nNeuron structure and artificial neural network. a Structure of biological neurons. b Mathematical inferring process of artificial neurons in multi-layer perceptron, including the input, weights, summation, activation function, and output. c Multi-layer perceptron artificial neural network\n\n\nPython, Manim and Machine Learning\nPython is a versatile programming language widely used for coding Artificial Neural Networks (ANNs) and Machine Learning (ML) algorithms.\n\nFourier Series\n\nThe Fourier series animation using Manim serves as an excellent example of how Python can be used to create complex visualizations and animations for mathematical concepts.\n\nVideo\nThe Fourier series animation showcases Python’s ability to visualize complex mathematical concepts, which is crucial in ML for understanding data distributions, model architectures, and algorithm behavior.\nSimilarly, when working with ANNs and ML, you would use Python to create visualizations of your model’s architecture, training progress, and prediction results.\nfrom manim import *\n\nclass FourierSeriesAnimation(Scene):\n    def construct(self):\n        # Create axes\n        axes = Axes(\n            x_range=[-2*PI, 2*PI, PI/2],\n            y_range=[-2, 2, 1],\n            axis_config={\"color\": BLUE},\n        )\n        \n        # Create the original function (square wave)\n        def square_wave(x):\n            return np.sign(np.sin(x))\n        \n        original_func = axes.plot(square_wave, color=WHITE)\n        \n        # Define a list of colors for the approximations\n        colors = [RED, GREEN, YELLOW, PURPLE, ORANGE]\n        \n        # Create Fourier series approximations\n        approximations = []\n        for n in range(1, 6):\n            def fourier_series(x):\n                return sum([(4 / ((2*k - 1) * PI)) * np.sin((2*k - 1) * x) for k in range(1, n+1)])\n            \n            approximations.append(axes.plot(fourier_series, color=colors[n-1]))\n        \n        # Add elements to the scene\n        self.add(axes, original_func)\n        \n        # Animate the Fourier series approximations\n        for approx in approximations:\n            self.play(Create(approx), run_time=2)\n            self.wait(1)\n        \n        self.wait(2)\n\n# Render the scene\nif __name__ == \"__main__\":\n    scene = FourierSeriesAnimation()\n    scene.render()\n\n\n\nPython, Quarto and Machine Learning\nQuarto supports executable Python code blocks within markdown.\nThis allows you to create fully reproducible documents and reports—the Python code required to produce your output is part of the document itself, and is automatically re-run whenever the document is rendered.\n\nmatplotlib demo\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis\n\n\n\n\n\n\n\n\nSpiking Neuron Potential\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulation parameters\nT = 100  # Total time in ms\ndt = 1   # Time step in ms\ntime = np.arange(0, T, dt)\n\n# Neuron parameters\nV_th = -50  # Spike threshold in mV\nV_reset = -65  # Reset potential in mV\nR = 1  # Resistance in MΩ\ntau = 10  # Membrane time constant in ms\n\n# Input current (constant for simplicity)\nI = 1.5  # Input current in μA\n\n# Initialize variables\nV_m = V_reset * np.ones(len(time))  # Membrane potential array\nspikes = []  # List to store spike times\n\n# Simulation loop\nfor t in range(1, len(time)):\n    dV = (-(V_m[t-1] - V_reset) + R * I) / tau * dt  # Update membrane potential\n    V_m[t] = V_m[t-1] + dV\n    \n    # Check for spike\n    if V_m[t] &gt;= V_th:\n        spikes.append(t)  # Record spike time\n        V_m[t] = V_reset  # Reset membrane potential after spike\n\n# Plotting results\nplt.figure(figsize=(10, 5))\nplt.plot(time, V_m, label=\"Membrane Potential ($V_m$)\", color='blue')\nplt.plot(spikes, [V_th]*len(spikes), 'ro', label=\"Spikes\")  # Plot spikes as red dots\nplt.axhline(V_th, color='black', linestyle='--', label=\"Spike Threshold\")\nplt.title(\"Membrane Potential of a Spiking Neuron\")\nplt.xlabel(\"Time (ms)\")\nplt.ylabel(\"Membrane Potential (mV)\")\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Membrane Potential of a Spiking Neuron\n\n\n\n\n\n\n\nPerceptron\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Activation function: Sigmoid\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n# Derivative of the sigmoid function\ndef sigmoid_derivative(x):\n    return x * (1 - x)\n\n# Input data (4 samples, 2 features)\nX = np.array([[0, 0],\n              [0, 1],\n              [1, 0],\n              [1, 1]])\n\n# Output data (XOR function)\ny = np.array([[0],\n              [1],\n              [1],\n              [0]])\n\n# Seed for reproducibility\nnp.random.seed(42)\n\n# Initialize weights\ninput_layer_neurons = 2  # Number of input features\nhidden_layer_neurons = 2  # Number of hidden neurons\noutput_neurons = 1        # Number of output neurons\n\n# Weights between input layer and hidden layer\nweights_input_hidden = np.random.uniform(size=(input_layer_neurons, hidden_layer_neurons))\n# Weights between hidden layer and output layer\nweights_hidden_output = np.random.uniform(size=(hidden_layer_neurons, output_neurons))\n\n# Learning rate\nlearning_rate = 0.5\n\n# Training the network\nepochs = 10000\nfor epoch in range(epochs):\n    # Forward pass\n    hidden_layer_activation = np.dot(X, weights_input_hidden)\n    hidden_layer_output = sigmoid(hidden_layer_activation)\n\n    output_layer_activation = np.dot(hidden_layer_output, weights_hidden_output)\n    predicted_output = sigmoid(output_layer_activation)\n\n    # Backpropagation\n    error = y - predicted_output\n    d_predicted_output = error * sigmoid_derivative(predicted_output)\n    \n    error_hidden_layer = d_predicted_output.dot(weights_hidden_output.T)\n    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n\n    # Updating weights\n    weights_hidden_output += hidden_layer_output.T.dot(d_predicted_output) * learning_rate\n    weights_input_hidden += X.T.dot(d_hidden_layer) * learning_rate\n\n# Final predictions after training\nfinal_hidden_layer_activation = np.dot(X, weights_input_hidden)\nfinal_hidden_layer_output = sigmoid(final_hidden_layer_activation)\n\nfinal_output_layer_activation = np.dot(final_hidden_layer_output, weights_hidden_output)\nfinal_predicted_output = sigmoid(final_output_layer_activation)\n\n# Plotting results\nplt.figure(figsize=(10, 5))\nplt.scatter(X[:, 0], X[:, 1], c=final_predicted_output.flatten(), cmap='RdYlBu', s=100)\nplt.title(\"Output of Two-Layer Perceptron (XOR Function)\")\nplt.xlabel(\"Input Feature 1\")\nplt.ylabel(\"Input Feature 2\")\nplt.colorbar(label='Predicted Output')\nplt.grid()\nplt.xlim(-0.5, 1.5)\nplt.ylim(-0.5, 1.5)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Output of Two-Layer Perceptron (XOR Function\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "about/about-roadmap.html",
    "href": "about/about-roadmap.html",
    "title": "Roadmap",
    "section": "",
    "text": "Step by step guide to becoming an AI and Data Scientist in 2024:\n\nAI and Data Scientist\n\nStep by step guide to becoming an AI Engineer in 2024:\n\nAI Engineer\n\n\n\n\n\n\n\n\n\n\n\n\n#\nDate\nTitle\nDescription\nTasks\n\n\n\n\n1\n2024-01-06\nPython Basics 1/4\nIntroduction to Python programming\nVariables, data types\n\n\n1\n2024-01-06\nML Foundations 1/4\nBasic machine learning concepts\nTypes of ML, learning paradigms\n\n\n1\n2024-01-06\nANN Foundation 1/4\nIntroduction to neural networks\nNetwork architecture basics\n\n\n2\n2024-01-13\nPython Basics 2/4\nControl structures\nLoops, conditionals, functions\n\n\n2\n2024-01-13\nML Foundations 2/4\nModel evaluation\nMetrics, validation techniques\n\n\n2\n2024-01-13\nANN Foundation 2/4\nForward propagation\nActivation functions, layers\n\n\n3\n2024-01-20\nPython Basics 3/4\nFunctions and modules\nFunction creation, importing modules\n\n\n3\n2024-01-20\nML Foundations 3/4\nData preprocessing\nCleaning, normalization, feature engineering\n\n\n3\n2024-01-20\nANN Foundation 3/4\nBackpropagation\nGradient descent basics\n\n\n4\n2024-01-27\nPython Basics 4/4\nError handling\nTry-except, debugging\n\n\n4\n2024-01-27\nML Foundations 4/4\nModel selection\nCross-validation, hyperparameter tuning\n\n\n4\n2024-01-27\nANN Foundation 4/4\nLoss functions\nCommon losses, optimization basics\n\n\n5\n2024-02-03\nPython Intermediate 1/4\nOOP concepts\nClasses, objects, inheritance\n\n\n5\n2024-02-03\nSupervised Learning 1/4\nLinear regression\nSimple and multiple regression\n\n\n5\n2024-02-03\nANN Training 1/4\nOptimization algorithms\nSGD, Adam, RMSprop\n\n\n6\n2024-02-10\nPython Intermediate 2/4\nAdvanced functions\nLambda, decorators, generators\n\n\n6\n2024-02-10\nSupervised Learning 2/4\nClassification basics\nLogistic regression, decision trees\n\n\n6\n2024-02-10\nANN Training 2/4\nRegularization\nL1, L2, dropout\n\n\n7\n2024-02-17\nPython Intermediate 3/4\nFile handling\nReading/writing files, JSON\n\n\n7\n2024-02-17\nSupervised Learning 3/4\nEnsemble methods\nRandom forests, boosting\n\n\n7\n2024-02-17\nANN Training 3/4\nBatch normalization\nImplementation and effects\n\n\n8\n2024-02-24\nPython Intermediate 4/4\nPackage management\nPip, virtual environments\n\n\n8\n2024-02-24\nSupervised Learning 4/4\nModel deployment\nSaving, loading, serving models\n\n\n8\n2024-02-24\nANN Training 4/4\nModel evaluation\nMetrics, validation strategies\n\n\n9\n2024-03-02\nPython for DS 1/4\nNumPy basics\nArrays, operations, indexing\n\n\n9\n2024-03-02\nDeep Learning 1/4\nDL frameworks\nPyTorch basics\n\n\n9\n2024-03-02\nANN Advanced 1/4\nAdvanced architectures\nResNet, Inception\n\n\n10\n2024-03-09\nPython for DS 2/4\nPandas basics\nDataFrames, series, indexing\n\n\n10\n2024-03-09\nDeep Learning 2/4\nData loading\nDatasets, dataloaders\n\n\n10\n2024-03-09\nANN Advanced 2/4\nTransfer learning\nPre-trained models, fine-tuning\n\n\n11\n2024-03-16\nPython for DS 3/4\nData visualization\nMatplotlib, Seaborn\n\n\n11\n2024-03-16\nDeep Learning 3/4\nCustom layers\nBuilding network components\n\n\n11\n2024-03-16\nANN Advanced 3/4\nAdvanced training\nLearning rate scheduling\n\n\n12\n2024-03-23\nPython for DS 4/4\nData analysis\nEDA, statistical analysis\n\n\n12\n2024-03-23\nDeep Learning 4/4\nModel optimization\nPerformance tuning\n\n\n12\n2024-03-23\nANN Advanced 4/4\nDeployment\nProduction considerations\n\n\n13\n2024-03-30\nLinear Algebra 1/4\nVectors and matrices\nBasic operations\n\n\n13\n2024-03-30\nCNN 1/4\nCNN basics\nConvolution operations\n\n\n13\n2024-03-30\nANN Applications 1/4\nImage classification\nMNIST implementation\n\n\n14\n2024-04-06\nLinear Algebra 2/4\nMatrix operations\nMultiplication, inverse\n\n\n14\n2024-04-06\nCNN 2/4\nPooling layers\nMax pooling, average pooling\n\n\n14\n2024-04-06\nANN Applications 2/4\nObject detection\nYOLO implementation\n\n\n15\n2024-04-13\nLinear Algebra 3/4\nEigenvalues\nDecomposition\n\n\n15\n2024-04-13\nCNN 3/4\nModern architectures\nVGG, ResNet\n\n\n15\n2024-04-13\nANN Applications 3/4\nSegmentation\nU-Net implementation\n\n\n16\n2024-04-20\nLinear Algebra 4/4\nPCA\nDimensionality reduction\n\n\n16\n2024-04-20\nCNN 4/4\nTransfer learning\nFine-tuning CNNs\n\n\n16\n2024-04-20\nANN Applications 4/4\nStyle transfer\nNeural style transfer\n\n\n17\n2024-04-27\nStatistics 1/4\nProbability basics\nDistributions\n\n\n17\n2024-04-27\nRNN 1/4\nRNN basics\nSequential data\n\n\n17\n2024-04-27\nRAG Basics 1/4\nVector databases\nEmbedding basics\n\n\n18\n2024-05-04\nStatistics 2/4\nHypothesis testing\nT-tests, chi-square\n\n\n18\n2024-05-04\nRNN 2/4\nLSTM\nLong-term dependencies\n\n\n18\n2024-05-04\nRAG Basics 2/4\nRetrieval methods\nVector similarity\n\n\n19\n2024-05-11\nStatistics 3/4\nRegression analysis\nStatistical modeling\n\n\n19\n2024-05-11\nRNN 3/4\nGRU\nGated recurrent units\n\n\n19\n2024-05-11\nRAG Basics 3/4\nQuery processing\nEmbedding generation\n\n\n20\n2024-05-18\nStatistics 4/4\nBayesian stats\nProbabilistic modeling\n\n\n20\n2024-05-18\nRNN 4/4\nAttention mechanism\nSelf-attention basics\n\n\n20\n2024-05-18\nRAG Basics 4/4\nResponse generation\nCombining retrieved info\n\n\n21\n2024-05-25\nBig Data 1/4\nDistributed systems\nHadoop ecosystem\n\n\n21\n2024-05-25\nLab: Classification 1/4\nProject setup\nData preparation\n\n\n21\n2024-05-25\nRAG Implementation 1/4\nSystem design\nArchitecture planning\n\n\n22\n2024-06-01\nBig Data 2/4\nMapReduce\nProgramming paradigm\n\n\n22\n2024-06-01\nLab: Classification 2/4\nModel development\nTraining pipeline\n\n\n22\n2024-06-01\nRAG Implementation 2/4\nIntegration\nConnecting components\n\n\n23\n2024-06-08\nBig Data 3/4\nSpark basics\nRDD operations\n\n\n23\n2024-06-08\nLab: Classification 3/4\nModel evaluation\nPerformance analysis\n\n\n23\n2024-06-08\nRAG Implementation 3/4\nOptimization\nPerformance tuning\n\n\n24\n2024-06-15\nBig Data 4/4\nSpark ML\nML pipelines\n\n\n24\n2024-06-15\nLab: Classification 4/4\nDeployment\nProduction readiness\n\n\n24\n2024-06-15\nRAG Implementation 4/4\nDeployment\nSystem deployment\n\n\n25\n2024-06-22\nLab: NLP 1/4\nText preprocessing\nTokenization, cleaning\n\n\n25\n2024-06-22\nLab: CV 1/4\nImage preprocessing\nAugmentation pipeline\n\n\n25\n2024-06-22\nLab: Full Stack 1/4\nFrontend design\nUI/UX planning\n\n\n26\n2024-06-29\nLab: NLP 2/4\nModel architecture\nBERT implementation\n\n\n26\n2024-06-29\nLab: CV 2/4\nModel development\nCNN architecture\n\n\n26\n2024-06-29\nLab: Full Stack 2/4\nBackend setup\nAPI development\n\n\n27\n2024-07-06\nLab: NLP 3/4\nFine-tuning\nTransfer learning\n\n\n27\n2024-07-06\nLab: CV 3/4\nTraining\nModel optimization\n\n\n27\n2024-07-06\nLab: Full Stack 3/4\nIntegration\nAPI endpoints\n\n\n28\n2024-07-13\nLab: NLP 4/4\nDeployment\nProduction system\n\n\n28\n2024-07-13\nLab: CV 4/4\nDeployment\nModel serving\n\n\n28\n2024-07-13\nLab: Full Stack 4/4\nDeployment\nSystem launch\n\n\n29\n2024-07-20\nFinal Project 1/4\nProject planning\nRequirements analysis\n\n\n29\n2024-07-20\nFinal Project 2/4\nImplementation\nCore development\n\n\n29\n2024-07-20\nFinal Project 3/4\nTesting\nSystem validation\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "About",
      "Roadmap"
    ]
  },
  {
    "objectID": "books/books-index.html",
    "href": "books/books-index.html",
    "title": "Books",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n\n\n Back to top",
    "crumbs": [
      "Book Resources",
      "Books"
    ]
  },
  {
    "objectID": "about/about-index.html",
    "href": "about/about-index.html",
    "title": "About",
    "section": "",
    "text": "Complementary to albertprofe.dev\n\n\n\n Back to top",
    "crumbs": [
      "About",
      "About"
    ]
  },
  {
    "objectID": "labs/labs-index.html",
    "href": "labs/labs-index.html",
    "title": "Labs",
    "section": "",
    "text": "Lab#ANN001: Building ANN recognizes numbers\n\nBuilding a neural network FROM SCRATCH (no Tensorflow/Pytorch, just numpy & math)\nKaggle notebook with all the code\nBlog article with more/clearer math explanation\n\n\n\nLab#GPT001: Python extractor\nAI reads books: Page-by-Page PDF Knowledge Extractor & Summarizer\n\nThe read_books.py script performs an intelligent page-by-page analysis of PDF books, methodically extracting knowledge points and generating progressive summaries at specified intervals. It processes each page individually, allowing for detailed content understanding while maintaining the contextual flow of the book.\n\n\nPython extractor\n\n\n\nLab#ANN002: Tensor for back propagation\nA small auto-grad engine (inspired from Karpathy's micrograd and PyTorch) using Apple's MLX and Numpy. That’s why the name: smolgrad\n\nIt will help y’all understand the core concepts behind automatic differentiation and backpropagation. I mean, AI is literally powered by these things.\n\nwhat is autograd?\nAuto-grad, short for automatic differentiation, is a technique used in machine learning to efficiently compute gradients of complex functions. In the context of neural networks, autograd enables the computation of gradients with respect to the model’s parameters, which is crucial for training the network using optimization algorithms like gradient descent.\n\nIt works by constructing a computational graph that represents the flow of data through the network. Each node in the graph represents a mathematical operation, and the edges represent the flow of data between these operations. By tracking the operations and their dependencies, autograd can automatically compute the gradients using the chain rule of calculus.\n\nAn algorithm used in conjuction with autograd is Backpropagation to train neural networks. It is the process of propagating the gradients backward through the computational graph, from the output layer to the input layer, in order to update the model’s parameters. This is the stuff which makes ML/DL models “learn”.\n\nautograd: repo\n\n\n\nLab#RAG001: Hedge fund trading team\nThis lab outlines the creation of a hedge fund trading team using AI agents. The process involves setting up multiple specialized agents to handle different aspects of trading:\n\nA Market Data Agent to gather and process financial data\nA Quant Agent for developing trading strategies\nA Risk Management Agent to assess and mitigate risks\nA Portfolio Management Agent for overall fund management\n\nThese agents are then integrated into an Agent Graph, allowing them to collaborate and share information efficiently.\n\nThe lab also covers obtaining stock price data and generating trading signals, which are crucial for making informed investment decisions.\n\nA key component of the lab is the creation of a Backtester. This tool allows the team to simulate and evaluate their trading strategies using historical data, providing valuable insights into potential performance without risking real capital.\nThe final step involves running the backtest to assess the effectiveness of the developed strategies and the overall performance of the AI-driven hedge fund team. This approach combines advanced AI techniques with traditional financial analysis, potentially offering a powerful new paradigm for hedge fund management.\n\nSyllabus\n\nSetup\nCreate Market Data Agent\nCreate Quant Agent\nCreate Risk Management Agent\nCreate Portfolio Management Agent\nCreate Agent Graph\nGet Stock Price and Trading Signals\nCreate Backtester\nRun the Backtest\n\n\n\nCitations\n\nHow Hedge Fund Operations Are Organized\nI Gave an AI Hedge Fund $1000 to invest with\nHedge Fund Strategies Introduction\nHedge Fund Trading Team Gist\nHedge Fund Structure\nHedge Fund Performance Evaluation Using Data Envelopment Analysis\nHow to Set Up a Hedge Fund\nFour dimensions of risk management for hedge funds\nhedge-fund-trading-team.ipynb\n\n\n\n\nLab#MAL001: How to structure your ML code\nML apps, like any other piece of software, can only generate business value once they are deployed and used in a production environment.\nAnd the thing is, deploying all-in-one messy Jupyter notebooks from your local machine to a production environment is neither easy, nor recommended from an MLOps perspective.\nOften a DevOps or MLOps senior colleague needs to re-write your all-in-on messy notebook, which adds excessive friction and frustration for you and the guy helping you.\nSo the question is:\n\nIs there a better way to develop and package your ML code, so you ship faster and better?\n\nYes, there is\n\nHow to structure your ML code\n\n\n\nLab#MAL002: How to structure your ML code\nThis is the first article in the series where we will build AI Agents from scratch without using any LLM orchestration frameworks. In this one you will learn:\n\nWhat are agents?\nHow the Tool usage actually works.\nHow to build a decorator wrapper that extracts relevant details from a Python function to be passed to the LLM via system prompt.\nHow to think about constructing effective system prompts that can be used for Agents.\nHow to build an Agent class that is able to plan and execute actions using provided Tools.\n\nYou can find the code examples for this and following projects in this repo\n\nBuilding AI Agents from scratch - Part 1: Tool use\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Lab Exercises",
      "Labs"
    ]
  },
  {
    "objectID": "resources/resources-index.html",
    "href": "resources/resources-index.html",
    "title": "Resources",
    "section": "",
    "text": "Math and Physics\n\nPhysics and Deep Learning\n\nbook Welcome to the Physics-based Deep Learning Book (v0.2)\nRob J Hyndman | Professor of Statistics\n\n\n\nAlgorithms\nMIT’s Introduction to Algorithms:\n\nLecture Notes\nLecture Videos\n\n\n\n\nIntroduction\n\nNeural Network\n\nA Brief Introduction to Neural Networks\nMaking neural nets uncool again\n3blue1brown github videos\n3blue1brown\nrepo manim\nBut what is a neural network? | Deep learning chapter 1\n\n\n\nDeep Learning\n\nMIT Introduction to Deep Learning | 6.S191\nPractical Deep Learning\nMIT 6.S191 Introduction to Deep Learning 2025\nMIT 6.S191 Introduction to Deep Learning 2024\n\n\n\nGenerative AI\n\nGenerative AI in a Nutshell - how to survive and thrive in the age of AI\nGenerative AI in a Nutshell: poster\n\n\n\nMachine Learning\n\nyoutube Learn Machine Learning Like a GENIUS and Not Waste Time\n6.867 Machine Learning (Fall 2004)\n6.867 | Fall 2006 | GraduateMachine Learning\nMachine Learning Resources\n\n\n\n\nTools, papers and labs\n\npaper Attention Is All You Need\nweb IT-Tools\nyoutube Building a neural network FROM SCRATCH (no Tensorflow/Pytorch, just numpy & math)\nAI by hand\npaper Deep learning Yann LeCun1,2, Yoshua Bengio3 & Geoffrey Hinton4,\n\n\n\nWorkflows and automation\n\nn8n\nBuild LLM Apps Easily\nAbacus\n\nto classify:\n\nbook Welcome to the Physics-based Deep Learning Book (v0.2)\nLLM Engineer’s Handbook: Master the art of engineering large language models from concept to production\nRepo\nThe Little Book of Deep Learning\nUnderstand Deep Learning: Python notebooks covering the whole text\nDeep Learning An MIT Press book Ian Goodfellow and Yoshua Bengio and Aaron Courville\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Additional Resources",
      "Resources"
    ]
  },
  {
    "objectID": "algorithms/algo-index.html",
    "href": "algorithms/algo-index.html",
    "title": "Algorithms",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n\n\n Back to top",
    "crumbs": [
      "What is Algorithms?",
      "Algorithms"
    ]
  },
  {
    "objectID": "deep/deep-index.html",
    "href": "deep/deep-index.html",
    "title": "Deep Learning",
    "section": "",
    "text": "Deep learning is a transformative subset of machine learning that utilizes artificial neural networks (ANNs) to model complex patterns in data, inspired by the human brain’s architecture.\nCharacterized by its ability to process vast amounts of information through multiple interconnected layers, deep learning has driven significant advancements in various fields, including computer vision, natural language processing, and automation.[1][2]\n\n\nThe rise of deep learning has enabled machines to achieve performance levels that surpass human capabilities in tasks such as image recognition and language translation, marking a pivotal shift in how artificial intelligence (AI) is applied across industries.[1]\nNotably, deep learning models, particularly deep neural networks (DNNs), have demonstrated remarkable success in diverse applications, such as Convolutional Neural Networks (CNNs) for image analysis and Recurrent Neural Networks (RNNs) for sequential data tasks.[3][4]\nHowever, the technology is not without its challenges; issues related to data quality, the demand for skilled professionals, and the interpretability of complex models pose significant hurdles for practitioners and organizations aiming to leverage deep learning effectively.[5][6]\nMoreover, ethical concerns regarding bias and transparency in AI decision-making processes have sparked ongoing debates about the responsible development and deployment of deep learning technologies.[7][8]\n\n\n\n\n\n\n\nAI fields\n\n\nAs industries increasingly rely on deep learning to enhance efficiency and innovation, understanding its foundational principles, capabilities, and limitations has become essential. The architecture of deep learning models involves intricate design choices, including layer composition and training algorithms like backpropagation, which are crucial for optimizing performance.[2][9]\n\nWith continuous research and development, the field of deep learning is poised for further growth, presenting both opportunities and challenges that must be addressed to harness its full potential in a responsible manner.\n\n\n\nDeep learning encompasses various types of models designed to address different types of data and tasks. Each model type has its unique architecture and application area, making them suitable for specific use cases.\n\n\n\n\n\n\n\nA comparative view of AI, machine learning, deep learning, and generative AI (source [16]).\n\n\n\nA comparative view of AI, machine learning, deep learning, and generative AI (source [16]).\n\nDeep Neural Networks and Generative AI\nDeep neural networks (DNNs) extend traditional neural networks by adding multiple hidden layers, which allows them to learn more complex patterns within the data. As of 2017, DNNs typically contained thousands to millions of units and connections, enabling them to perform tasks such as image recognition and natural language processing at levels that can surpass human capabilities[1]\nEach layer in a DNN builds on the previous one, allowing the model to capture intricate patterns through its architecture, which can involve convolutional layers for spatial data processing, pooling layers for dimensionality reduction, and fully connected layers for final classification tasks[9][2]\n\nGenerative Adversarial Networks (GANs): Generative Adversarial Networks (GANs) consist of two neural networks: a generator and a discriminator, which are trained simultaneously in a competitive framework.\n\nThe generator aims to create realistic synthetic data, while the discriminator evaluates the authenticity of the generated data against real data samples.\nThis adversarial training process helps improve the quality of generated outputs, making GANs popular in applications such as image generation, video creation, and data augmentation. Their ability to produce high-quality visuals has significantly impacted fields like art, entertainment, and gaming[11][12].\n\nTransformer Models: Transformer models have gained prominence in natural language processing tasks due to their efficiency in handling sequential data without relying on recurrence.\n\nTransformers utilize self-attention mechanisms to weigh the influence of different words in a sentence, allowing for a better understanding of context and relationships within the text.\nThis architecture has led to significant advancements in machine translation, text summarization, and conversational AI applications, making Transformers a crucial development in the deep learning landscape[3].",
    "crumbs": [
      "Introduction to Deep Learning",
      "Deep Learning"
    ]
  },
  {
    "objectID": "machine/machine-index.html",
    "href": "machine/machine-index.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Introduction\nMachine learning (ML) is a branch of artificial intelligence (AI) focused on enabling computers and machines to imitate the way that humans learn, to perform tasks autonomously, and to improve their performance and accuracy through experience and exposure to more data.\nUC Berkeley breaks out the learning system of a machine learning algorithm into three main parts.\n\nA Decision Process: In general, machine learning algorithms are used to make a prediction or classification. Based on some input data, which can be labeled or unlabeled, your algorithm will produce an estimate about a pattern in the data.\nAn Error Function: An error function evaluates the prediction of the model. If there are known examples, an error function can make a comparison to assess the accuracy of the model.\nA Model Optimization Process: If the model can fit better to the data points in the training set, then weights are adjusted to reduce the discrepancy between the known example and the model estimate. The algorithm will repeat this iterative “evaluate and optimize” process, updating weights autonomously until a threshold of accuracy has been met.\nWhat is machine learning?\n\n\nMachine learning is a subfield of artificial intelligence, which is broadly defined as the capability of a machine to imitate intelligent human behavior. Artificial intelligence systems are used to perform complex tasks in a way that is similar to how humans solve problems.\n\n\n\nDeep Learning and Machine Learning\n\n\n\nFrom: Optical neural networks: progress and challenges\n\n\nNeuron structure and artificial neural network. a Structure of biological neurons. b Mathematical inferring process of artificial neurons in multi-layer perceptron, including the input, weights, summation, activation function, and output. c Multi-layer perceptron artificial neural network\n\n\nMachine Learning fields\nThe core idea that classifies Deep Learning as a field of Machine Learning (ML) lies in its use of artificial neural networks (ANNs) with multiple layers to automatically learn patterns and representations from data. Unlike traditional ML, which often requires manual feature engineering, deep learning models extract features hierarchically, enabling them to handle complex tasks such as image recognition and natural language processing.\nKey fields in Machine Learning:\n\nSupervised Learning\nUnsupervised Learning\nSemi-Supervised Learning\nReinforcement Learning\n\nSupervised Learning, Unsupervised Learning, Semi-Supervised Learning, and Reinforcement Learning are key paradigms in machine learning. Each approach addresses different types of problems and data availability.\nSupervised learning uses labeled data for training, while unsupervised learning identifies patterns in unlabeled data. Semi-supervised learning combines both, and reinforcement learning focuses on decision-making through trial and error in dynamic environments.\n\n\n\n\n\n\n\n\n\n\n\nSubfield\nDefinition\nObjective\nExamples\nApplications\n\n\n\n\nSupervised Learning\nTrains a model on a labeled dataset where each training example is paired with an output label.\nLearn a mapping from inputs to outputs for predictions on unseen data.\nLinear regression, logistic regression, decision trees, SVMs\nClassification (e.g., spam detection), regression (e.g., house prices)\n\n\nUnsupervised Learning\nTrains a model on data without labeled outputs to learn the underlying structure or distribution of the data.\nIdentify patterns, groupings, or relationships within the data.\nClustering (e.g., k-means, hierarchical), dimensionality reduction (e.g., PCA)\nMarket segmentation, anomaly detection, exploratory data analysis\n\n\nSemi-Supervised Learning\nCombines both labeled and unlabeled data for training, typically with a small amount of labeled data.\nImprove learning accuracy by leveraging additional unlabeled data while benefiting from labeled examples.\nTechniques using supervised methods enhanced by clustering.\nScenarios where labeling data is expensive or time-consuming (e.g., image classification)\n\n\nReinforcement Learning\nInvolves training an agent to make decisions by interacting with an environment and learning through trial and error.\nLearn a policy that maximizes cumulative rewards over time.\nQ-learning, deep reinforcement learning (using neural networks)\nRobotics, game playing (e.g., AlphaGo), autonomous systems\n\n\n\n\n\n\nDeep Learning as distinct field\nDeep learning is indeed considered a distinct field within the broader domain of machine learning.\nIt is characterized by its use of deep neural networks to learn complex patterns from large datasets.\n\n\n\n\n\n\nDeep Learning as a Distinct Field\n\n\n\nCore Concept: Deep learning utilizes artificial neural networks with multiple layers (deep architectures) to automatically learn representations from data, distinguishing it from traditional machine learning methods that often rely on manual feature extraction.\nComplexity Handling: Deep learning excels at managing high-dimensional data and capturing non-linear relationships, making it particularly effective for tasks like image and speech recognition.\n\n\n\nRelationship to Other Machine Learning Subfields\nDeep learning can be viewed as a specialized approach within the following main subfields of machine learning, here’s how deep learning fits into the classification of machine learning and its relationship with other fields:\n\nSupervised Learning:\n\nDefinition: Involves training models on labeled datasets where the output is known.\nDeep Learning Application: Deep learning models, such as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), are frequently used for supervised tasks like classification and regression.\n\nUnsupervised Learning:\n\nDefinition: Involves training models on unlabeled data to discover hidden patterns or structures.\nDeep Learning Application: Techniques like autoencoders and generative adversarial networks (GANs) are used in deep learning for tasks such as clustering and anomaly detection.\n\nSemi-Supervised Learning:\n\nDefinition: Combines both labeled and unlabeled data for training, leveraging the strengths of both approaches.\nDeep Learning Application: Deep learning can enhance semi-supervised methods by using pre-trained models to extract features from unlabeled data.\n\nReinforcement Learning:\n\nDefinition: Involves training agents to make decisions by interacting with an environment, receiving feedback in the form of rewards or penalties.\nDeep Learning Application: Deep reinforcement learning combines neural networks with reinforcement learning principles, enabling agents to learn complex behaviors in environments with high-dimensional state spaces.\n\n\n\nDeep learning represents a significant advancement in machine learning, offering powerful tools for both supervised and unsupervised tasks. Its ability to automatically learn features from raw data sets it apart from traditional machine learning methods, making it a critical area of research and application in modern AI systems.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Machine Learning Basics",
      "Machine Learning"
    ]
  },
  {
    "objectID": "resources/resources-books.html",
    "href": "resources/resources-books.html",
    "title": "Books cards",
    "section": "",
    "text": "Reading is essential for those who seek to rise above the ordinary. - Jim Rohn\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nA Brief Introduction to Neural Networks\n\n\nArtificial Neural Networks\n\n\n\nartificial neural networks\n\n\nmachine learning\n\n\n\nAn introduction to the fundamentals of neural networks and their applications in artificial intelligence.\n\n\n\n\n\nWednesday, January 1, 2025\n\n\nDavid Kriesel\n\n\n5 min\n\n\nThursday, January 2, 2025\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Books",
      "Books cards"
    ]
  },
  {
    "objectID": "python/python-index.html#about-this-course",
    "href": "python/python-index.html#about-this-course",
    "title": "Python",
    "section": "",
    "text": "This is the course material page for the Introduction to Programming course (BSCS1001, 5 ECTS) and the Advanced Course in Programming (BSCS1002, 5 ECTS) from the Department of Computer Science at the University of Helsinki.",
    "crumbs": [
      "Python Programming",
      "Python"
    ]
  },
  {
    "objectID": "python/python-index.html#lectures",
    "href": "python/python-index.html#lectures",
    "title": "Python",
    "section": "",
    "text": "There are no more live lectures for this year’s MOOC instance. The lecture recordings can be viewed on the table below.\n\nLectures",
    "crumbs": [
      "Python Programming",
      "Python"
    ]
  },
  {
    "objectID": "python/python-index.html#links",
    "href": "python/python-index.html#links",
    "title": "Python",
    "section": "",
    "text": "Python Programming MOOC 2024\nAll exercises\nPart 1: Getting started",
    "crumbs": [
      "Python Programming",
      "Python"
    ]
  },
  {
    "objectID": "python/python-part1.html",
    "href": "python/python-part1.html",
    "title": "Python Helsinki: 1",
    "section": "",
    "text": "Getting started\n- Part 1: Getting started\n\n\n\n\n Back to top",
    "crumbs": [
      "Python Programming",
      "Python Helsinki: 1"
    ]
  },
  {
    "objectID": "python/python-part2.html",
    "href": "python/python-part2.html",
    "title": "Python Helsinki: 2",
    "section": "",
    "text": "Programming terminology\n- Part 2: Programming terminology\n\n\n\n\n Back to top",
    "crumbs": [
      "Python Programming",
      "Python Helsinki: 2"
    ]
  },
  {
    "objectID": "python/python-part3.html",
    "href": "python/python-part3.html",
    "title": "Python Helsinki: 3",
    "section": "",
    "text": "More about variables\n- Part 3: More about variables\n\n\n\n\n Back to top",
    "crumbs": [
      "Python Programming",
      "Python Helsinki: 3"
    ]
  },
  {
    "objectID": "python/python-part4.html",
    "href": "python/python-part4.html",
    "title": "Python Helsinki: 4",
    "section": "",
    "text": "Functions\n- Part 4: Functions\n\n\n\n\n Back to top",
    "crumbs": [
      "Python Programming",
      "Python Helsinki: 4"
    ]
  },
  {
    "objectID": "python/python-part5.html",
    "href": "python/python-part5.html",
    "title": "Python Helsinki: 5",
    "section": "",
    "text": "Lists\n- Part 5: Lists\n\n\n\n\n Back to top",
    "crumbs": [
      "Python Programming",
      "Python Helsinki: 5"
    ]
  },
  {
    "objectID": "python/python-part6.html",
    "href": "python/python-part6.html",
    "title": "Python Helsinki: 6",
    "section": "",
    "text": "File I/O and Strings\n- Part 6: File I/O and Strings\n\n\n\n\n Back to top",
    "crumbs": [
      "Python Programming",
      "Python Helsinki: 6"
    ]
  },
  {
    "objectID": "python/python-part7.html",
    "href": "python/python-part7.html",
    "title": "Python Helsinki: 7",
    "section": "",
    "text": "Tuples\n- Part 7: Tuples\n\n\n\n\n Back to top",
    "crumbs": [
      "Python Programming",
      "Python Helsinki: 7"
    ]
  },
  {
    "objectID": "python/python-part8.html",
    "href": "python/python-part8.html",
    "title": "Python Helsinki: 8",
    "section": "",
    "text": "Dictionaries\n- Part 8: Dictionaries\n\n\n\n\n Back to top",
    "crumbs": [
      "Python Programming",
      "Python Helsinki: 8"
    ]
  },
  {
    "objectID": "python/python-part9.html",
    "href": "python/python-part9.html",
    "title": "Python Helsinki: 9",
    "section": "",
    "text": "Objects and Classes\n- Part 9: Objects and Classes\n\n\n\n\n Back to top",
    "crumbs": [
      "Python Programming",
      "Python Helsinki: 9"
    ]
  },
  {
    "objectID": "python/python-part10.html",
    "href": "python/python-part10.html",
    "title": "Python Helsinki: 10",
    "section": "",
    "text": "Object-Oriented Programming\n- Part 10: Object-Oriented Programming\n\n\n\n\n Back to top",
    "crumbs": [
      "Python Programming",
      "Python Helsinki: 10"
    ]
  },
  {
    "objectID": "python/python-part11.html",
    "href": "python/python-part11.html",
    "title": "Python Helsinki: 11",
    "section": "",
    "text": "Advanced Object-Oriented Programming\n- Part 11: Advanced Object-Oriented Programming\n\n\n\n\n Back to top",
    "crumbs": [
      "Python Programming",
      "Python Helsinki: 11"
    ]
  },
  {
    "objectID": "python/python-part12.html",
    "href": "python/python-part12.html",
    "title": "Python Helsinki: 12",
    "section": "",
    "text": "Libraries and Modules\n- Part 12: Libraries and Modules\n\n\n\n\n Back to top",
    "crumbs": [
      "Python Programming",
      "Python Helsinki: 12"
    ]
  },
  {
    "objectID": "python/python-part13.html",
    "href": "python/python-part13.html",
    "title": "Python Helsinki: 13",
    "section": "",
    "text": "Data Processing\n- Part 13: Data Processing\n\n\n\n\n Back to top",
    "crumbs": [
      "Python Programming",
      "Python Helsinki: 13"
    ]
  },
  {
    "objectID": "python/python-part14.html",
    "href": "python/python-part14.html",
    "title": "Python Helsinki: 14",
    "section": "",
    "text": "GUI Programming\n- Part 14: GUI Programming\n\n\n\n\n Back to top",
    "crumbs": [
      "Python Programming",
      "Python Helsinki: 14"
    ]
  },
  {
    "objectID": "generative/generative-transformer.html",
    "href": "generative/generative-transformer.html",
    "title": "Generative AI Transformer",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n\n\n Back to top",
    "crumbs": [
      "Generative AI Transformer",
      "Generative AI Transformer"
    ]
  },
  {
    "objectID": "generative/generative-gpt.html",
    "href": "generative/generative-gpt.html",
    "title": "Generative AI GPT",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n\n\n Back to top",
    "crumbs": [
      "Generative LLM",
      "Generative AI GPT"
    ]
  },
  {
    "objectID": "generative/generative-index.html",
    "href": "generative/generative-index.html",
    "title": "Generative AI",
    "section": "",
    "text": "Generative AI Models\n\nGenerative AI models are machine learning models that can generate new data from existing data.\n\nGenerative AI models are used in many applications, such as image generation, text generation, speech synthesis, and music generation.\nTable with key technologies and models that play crucial roles in Generative AI:\n\n\n\n\n\n\n\n\n\n\n\nTechnology\nYear Released\nDescription\nKey Differences\nApplications\n\n\n\n\nRetrieval-Augmented Generation (RAG)\n2020\nCombines information retrieval with text generation.\nFocuses on enhancing generation with retrieved data.\nChatbots, question answering systems\n\n\nGenerative Adversarial Networks (GANs)\n2014\nConsists of a generator and a discriminator competing against each other.\nUses adversarial training for realistic data generation.\nImage and video generation\n\n\nLarge Language Models (LLMs)\n2018\nDeep learning models trained on vast amounts of text data.\nSpecializes in understanding and generating human-like text.\nText generation, summarization, translation\n\n\nGPT (Generative Pre-trained Transformer)\n2018\nA specific type of LLM developed by OpenAI using transformer architecture.\nPre-trained on large datasets for coherent text generation.\nConversational agents, content creation\n\n\nAutoencoders\n1987\nNeural networks for learning efficient data representations.\nComposed of an encoder and decoder for data compression.\nDenoising, anomaly detection\n\n\nTransformers\n2017\nNeural network architecture using self-attention mechanisms.\nDesigned for processing sequential data effectively.\nNatural language processing, image analysis\n\n\n\n\n\n\n\nComparison of three categories of generative models.\n\n\n\n\nVariational Autoencoders (VAEs)\n\nVariational AutoEncoders (VAEs) is a type of autoencoder that extends the basic architecture to learn a probabilistic model of the data.\n\nThis allows them to generate new data similar to the original input but not identical.\nThe key innovation in VAEs is the introduction of a regularization term known as the Kullback-Leibler (KL) divergence, which encourages the learned distribution to match a prior distribution, typically a standard normal distribution.\nThis regularization term allows VAEs to generate more diverse and realistic data than traditional autoencoders.\n\n\n\nIllustration of variational autoencoder model\n\n\n\nFrom Autoencoder to Beta-VAE\n\n\n\nGenerative Adversarial Networks (GANs)\nGenerative Adversarial Networks (GANs) were introduced by Ian Goodfellow in 2014. They are a class of generative models that use two neural networks, a generator, and a discriminator, to generate new data.\n\nThe generator network takes a random noise vector as input and generates a sample from the data distribution. The discriminator network takes a sample from the data distribution and a sample from the generator network and tries to distinguish between them.\n\nThe generator network is trained to fool the discriminator network, while the discriminator network is trained to distinguish between real and fake samples.\n\n\n\nIllustration of GAN model\n\n\n\nFrom GAN to WGAN\n\n\n\nTransformers\nThe Transformer was introduced in 2017 by Vaswani et al.\n\nIt is a neural network architecture that uses attention mechanisms to process data sequences.\n\nThe Transformer has been used in many applications, such as machine translation, text summarization, and image captioning.\n\n\n\nTransformer model architecture\n\n\n\nThe Transformer Blueprint: A Holistic Guide to the Transformer Neural Network Architecture\n\n\n\nLarge Language Models (LLM)\n\nLLMs are trained on vast amounts of textual data and use transformer models to analyze and produce coherent, contextually relevant language.\n\nThey excel at tasks such as text generation, translation, and answering questions based on their training data.\n\n\nResources\n\nAn overview of Generative AI in 2023\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Generative AI",
      "Generative AI"
    ]
  },
  {
    "objectID": "generative/generative-rag.html",
    "href": "generative/generative-rag.html",
    "title": "Generative AI RAG",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n\n\n Back to top",
    "crumbs": [
      "Generative RAG",
      "Generative AI RAG"
    ]
  },
  {
    "objectID": "generative/generative-llm.html",
    "href": "generative/generative-llm.html",
    "title": "Generative LLM",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n\n\n Back to top",
    "crumbs": [
      "Generative LLM",
      "Generative LLM"
    ]
  },
  {
    "objectID": "resources/books/articles/A_Brief_Introduction_to_Neural_Networks_DavidKriesel.html",
    "href": "resources/books/articles/A_Brief_Introduction_to_Neural_Networks_DavidKriesel.html",
    "title": "A Brief Introduction to Neural Networks",
    "section": "",
    "text": "A Brief Introduction to Neural Networks David Kriesel"
  },
  {
    "objectID": "resources/books/articles/A_Brief_Introduction_to_Neural_Networks_DavidKriesel.html#from-biology-to-formalization",
    "href": "resources/books/articles/A_Brief_Introduction_to_Neural_Networks_DavidKriesel.html#from-biology-to-formalization",
    "title": "A Brief Introduction to Neural Networks",
    "section": "From Biology to Formalization",
    "text": "From Biology to Formalization\nIntroduction, Motivation and History\nHow to teach a computer? You can either write a rigid program – or you can enable the computer to learn on its own. Living beings don’t have any programmer writing a program for developing their skills, which only has to be executed. They learn by themselves – without the initial experience of external knowledge – and thus can solve problems better than any computer today. KaWhat qualities are needed to achieve such a behavior for devices like computers? Can such cognition be adapted from biology? History, development, decline and resurgence of a wide approach to solve problems.\nBiological Neural Networks\nHow do biological systems solve problems? How is a system of neurons working? How can we understand its functionality? What are different quantities of neurons able to do? Where in the nervous system are information processed? A short biological overview of the complexity of simple elements of neural information processing followed by some thoughts about their simplification in order to technically adapt them.\nComponents of Artificial Neural Networks\nFormal definitions and colloquial explanations of the components that realize the technical adaptations of biological neural networks. Initial descriptions of how to combine these components to a neural network.\nHow to Train a Neural Network?\nApproaches and thoughts of how to teach machines. Should neural networks be corrected? Should they only be encouraged? Or should they even learn without any help? Thoughts about what we want to change during the learning procedure and how we will change it, about the measurement of errors and when we have learned enough."
  },
  {
    "objectID": "resources/books/articles/A_Brief_Introduction_to_Neural_Networks_DavidKriesel.html#supervised-learning-network-paradigms",
    "href": "resources/books/articles/A_Brief_Introduction_to_Neural_Networks_DavidKriesel.html#supervised-learning-network-paradigms",
    "title": "A Brief Introduction to Neural Networks",
    "section": "Supervised learning Network Paradigms",
    "text": "Supervised learning Network Paradigms\nThe Perceptron\nA classic among the neural networks. If we talk about a neural network, then in the majority of cases we speak about a percepton or a variation of it. Perceptrons are multi-layer networks without recurrence and with fixed input and output layers. Description of a perceptron, its limits and extensions that should avoid the limitations. Derivation of learning procedures and discussion about their problems.\nRadial Basis Functions\nRBF networks approximate functions by stretching and compressing Gaussians and then summing them spatially shifted. Description of their functions and their learning process. Comparison with multi-layer perceptrons.\nRecurrent Multi-layer Perceptrons\nSome thoughts about networks with internal states. Learning approaches using such networks, overview of their dynamics.\nHopfield Networks\nIn a magnetic field, each particle applies a force to any other particle so that all particles adjust their movements in the energetically most favorable way. This natural mechanism is copied to adjust noisy inputs in order to match their real models.\nLearning Vector Quantisation\nLearning vector quantization is a learning procedure with the aim to reproduce the vector training sets divided in predefined classes as good as possible by using a few representative vectors. If this has been managed, vectors which were unkown until then could easily be assigned to one of these classes."
  },
  {
    "objectID": "resources/books/articles/A_Brief_Introduction_to_Neural_Networks_DavidKriesel.html#unsupervised-learning-network-paradigms",
    "href": "resources/books/articles/A_Brief_Introduction_to_Neural_Networks_DavidKriesel.html#unsupervised-learning-network-paradigms",
    "title": "A Brief Introduction to Neural Networks",
    "section": "Unsupervised learning Network Paradigms",
    "text": "Unsupervised learning Network Paradigms\nSelf Organizing Feature Maps\nA paradigm of unsupervised learning neural networks, which maps an input space by its fixed topology and thus independently looks for simililarities. Function, learning procedure, variations and neural gas.\nAdaptive Resonance Theory\nAn ART network in its original form shall classify binary input vectors, i.e. to assign them to a 1-out-of-n output. Simultaneously, the so far unclassified patterns shall be recognized and assigned to a new class."
  },
  {
    "objectID": "resources/books/articles/A_Brief_Introduction_to_Neural_Networks_DavidKriesel.html#excursi-appendices-and-registers",
    "href": "resources/books/articles/A_Brief_Introduction_to_Neural_Networks_DavidKriesel.html#excursi-appendices-and-registers",
    "title": "A Brief Introduction to Neural Networks",
    "section": "Excursi, Appendices and Registers",
    "text": "Excursi, Appendices and Registers\nCluster Analysis and Regional and Online Learnable Fields\nIn Grimm’s dictionary the extinct German word “Kluster” is described by “was dicht und dick zusammensitzet (a thick and dense group of sth.)”. In static cluster analysis, the formation of groups within point clouds is explored. Introduction of some procedures, comparison of their advantages and disadvantages. Discussion of an adaptive clustering method based on neural networks. A regional and online learnable field models from a point cloud, possibly with a lot of points, a comparatively small set of neurons being representative for the point cloud.\nNeural Networks Used for Prediction\nDiscussion of an application of neural networks: A look ahead into the future of time series.\nReinforcement Learning\nWhat if there were no training examples but it would nevertheless be possible to evaluate how good we have learned to solve a problem? et us regard a learning paradigm that is situated between supervised and unsupervised learning."
  },
  {
    "objectID": "resources/books/articles/A_Brief_Introduction_to_Neural_Networks_DavidKriesel.html#additional-resources",
    "href": "resources/books/articles/A_Brief_Introduction_to_Neural_Networks_DavidKriesel.html#additional-resources",
    "title": "A Brief Introduction to Neural Networks",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nAuthor’s Website\nNeural Networks Research Group\nIEEE Computational Intelligence Society"
  },
  {
    "objectID": "resources/books/articles/A_Brief_Introduction_to_Neural_Networks_DavidKriesel.html#related-topics",
    "href": "resources/books/articles/A_Brief_Introduction_to_Neural_Networks_DavidKriesel.html#related-topics",
    "title": "A Brief Introduction to Neural Networks",
    "section": "Related Topics",
    "text": "Related Topics\n\nDeep Learning\nComputer Vision\nNatural Language Processing\nReinforcement Learning"
  },
  {
    "objectID": "resources/books/articles/A_Brief_Introduction_to_Neural_Networks_DavidKriesel.html#video-introduction-to-neural-networks",
    "href": "resources/books/articles/A_Brief_Introduction_to_Neural_Networks_DavidKriesel.html#video-introduction-to-neural-networks",
    "title": "A Brief Introduction to Neural Networks",
    "section": "Video Introduction to Neural Networks",
    "text": "Video Introduction to Neural Networks"
  },
  {
    "objectID": "ann/ann-index.html#overview",
    "href": "ann/ann-index.html#overview",
    "title": "ANN: Table of Contents",
    "section": "",
    "text": "Artificial Neural Networks (ANNs) are computational models inspired by biological neural networks in human brains. They are designed to process complex data and perform tasks like classification, prediction, and pattern recognition.",
    "crumbs": [
      "What is ANN?",
      "ANN: Table of Contents"
    ]
  },
  {
    "objectID": "ann/ann-index.html#architecture",
    "href": "ann/ann-index.html#architecture",
    "title": "ANN: Table of Contents",
    "section": "",
    "text": "Layers:\n\nInput Layer: Initial point of data entry\nHidden Layers: Process and analyze data\nOutput Layer: Provides final computational results",
    "crumbs": [
      "What is ANN?",
      "ANN: Table of Contents"
    ]
  },
  {
    "objectID": "ann/ann-index.html#types-of-neural-networks",
    "href": "ann/ann-index.html#types-of-neural-networks",
    "title": "ANN: Table of Contents",
    "section": "",
    "text": "Feedforward Neural Networks (FNNs)\n\nData moves in one direction\nNo cycles or loops\nStraightforward processing\n\nConvolutional Neural Networks (CNNs)\n\nSpecialized for processing grid data (images)\nUses convolutional layers\nEffective for image recognition\n\nRecurrent Neural Networks (RNNs)\n\nDesigned for sequence prediction\nMaintains context across inputs\nUseful for time-series and language processing",
    "crumbs": [
      "What is ANN?",
      "ANN: Table of Contents"
    ]
  },
  {
    "objectID": "ann/ann-index.html#training-methods",
    "href": "ann/ann-index.html#training-methods",
    "title": "ANN: Table of Contents",
    "section": "",
    "text": "Learning Approaches:\n\nSupervised Learning: Uses labeled training data\nUnsupervised Learning: Discovers patterns in unlabeled data\nReinforcement Learning: Learns through environmental interaction\n\nTraining Process:\n\nForward Propagation\nLoss Function Calculation\nBackward Propagation (Backpropagation)",
    "crumbs": [
      "What is ANN?",
      "ANN: Table of Contents"
    ]
  },
  {
    "objectID": "ann/ann-index.html#applications",
    "href": "ann/ann-index.html#applications",
    "title": "ANN: Table of Contents",
    "section": "",
    "text": "Key Areas:\n\nNatural Language Processing\nHealthcare\neCommerce\nComputer Vision",
    "crumbs": [
      "What is ANN?",
      "ANN: Table of Contents"
    ]
  },
  {
    "objectID": "ann/ann-index.html#challenges",
    "href": "ann/ann-index.html#challenges",
    "title": "ANN: Table of Contents",
    "section": "",
    "text": "Limitations:\n\nOverfitting\nLack of Interpretability\nHigh Training Data Requirements\nComputational Intensity\n\nEthical Considerations:\n\nBias in decision-making\nAccountability\nTransparency in automated systems",
    "crumbs": [
      "What is ANN?",
      "ANN: Table of Contents"
    ]
  },
  {
    "objectID": "ann/ann-index.html#ethical-considerations",
    "href": "ann/ann-index.html#ethical-considerations",
    "title": "Artificial Neural Networks",
    "section": "",
    "text": "Bias in decision-making\nAccountability\nTransparency in automated systems",
    "crumbs": [
      "What is ANN?",
      "Artificial Neural Networks"
    ]
  },
  {
    "objectID": "ann/ann-introduction.html",
    "href": "ann/ann-introduction.html",
    "title": "ANN: Introduction",
    "section": "",
    "text": "Artificial Neural Networks (ANNs) are computational models inspired by the biological neural networks found in human brains, designed to process complex data and perform tasks such as classification, prediction, and pattern recognition.\n\n\n\n\n\n\n\nThe Basic Layers in ANN\n\n\nTheir architecture typically comprises three layers:\n\nan input layer that receives data,\none or more hidden layers that perform processing through interconnected nodes (neurons),\nand an output layer that delivers the final result.\n\nANNs are widely recognized for their ability to learn from large datasets, making them pivotal in various applications, including image and speech recognition, natural language processing, and medical diagnosis[1][2].\n\n\nThe growing significance of ANNs stems from their versatility and effectiveness across multiple domains, from enhancing customer experiences in eCommerce through personalized recommendations to advancing medical technologies by aiding in disease diagnosis and treatment planning[3][4].\nHowever, their deployment also raises important ethical considerations, particularly regarding bias, accountability, and the implications of automated decision-making in sensitive fields such as healthcare.\nAs organizations increasingly integrate ANNs into their operations, issues of transparency and interpretability have emerged as critical concerns, challenging the trustworthiness of these models in high-stakes applications[5][6].\nVarious types of ANNs, including Feedforward Neural Networks (FNNs), Convolutional Neural Networks (CNNs), and Recurrent Neural Networks (RNNs), cater to different tasks and data structures, each offering unique advantages.\n\nFor instance, CNNs excel in image-related tasks by identifying spatial hierarchies in data, while RNNs are adept at processing sequential data, such as time series or natural language, by maintaining context across inputs[7][8].\n\nDespite their transformative potential, ANNs face challenges, including the risk of overfitting, the requirement for extensive training data, and the computational resources needed for effective training.\nFurthermore, the complex nature of ANNs often leads to their characterization as “black boxes,” making it difficult to interpret the decision-making processes involved.\n\n\nAs research progresses, addressing these limitations while promoting ethical practices will be essential to unlocking the full potential of ANNs across diverse applications[10][11].\n\n\n\nArtificial Neural Networks (ANNs) are structured to mimic the interconnected network of neurons in the human brain, facilitating complex information processing.\nThe architecture of an ANN typically consists of three main types of layers: the input layer, hidden layers, and the output layer.\n\nInput Layer\nThe input layer serves as the initial point of data entry for the neural network. It receives information from the external environment in various forms such as numbers, letters, audio files, or image pixels.\nEach node in the input layer corresponds to a specific feature of the data being processed, and it plays a critical role in transmitting this information to subsequent layers for further analysis[1][2].\nHidden Layers\nHidden layers are positioned between the input and output layers and are responsible for the majority of the processing within the ANN. A network may have one or multiple hidden layers, each containing numerous artificial neurons.\nThese layers perform complex mathematical operations on the input data, identifying patterns and relationships that are not immediately apparent. The output from one hidden layer becomes the input for the next layer, allowing for a deeper analysis of the data at each stage[1][3][2].\n\nTypes of Hidden Layers: Hidden layers can vary in their configuration and function. They may employ different types of activation functions, which introduce non-linear properties to the network, enabling it to learn complex mappings from inputs to outputs.\nThe choice of activation function can significantly impact the performance of the neural network in various tasks[4][5].\n\nOutput Layer\nThe output layer is the final layer in the ANN structure and provides the ultimate result of the computations performed by the network. Depending on the nature of the task, the output layer can contain a single node for binary classification or multiple nodes for multi-class classification problems.\nEach output node generates a response based on the processed information, translating the network’s findings into actionable insights or predictions[1][3][5].\n\n\n\nArtificial Neural Networks (ANNs) can be categorized into several key types, each designed for specific tasks and applications.\n\n\n\n\n\n\n\nFigure 1. Illustration of various types of artificial neural networks (ANN) and their associated components\n\n\n\n\nFigure 1. Illustration of various types of artificial neural networks (ANN) and their associated components\n\n(A) A basic ANN consists of an input layer (red circles), one or more hidden layers (peach circles), and an output layer (blue circle). In the case of neuronal modelling, the input could be features such as the membrane potential (V m ), and the excitatory (exc) and inhibitory (inh) synaptic inputs. The hidden layers perform computations on the inputs, with the actual operations depending on the type of ANN. Their objective is to identify features in the inputs and use these to correlate a given input and the correct output. An ANN can have multiple outputs: in this example, the output is a prediction of the membrane potential. (B) A deep neural network (DNN) is an ANN with multiple hidden layers. (C) A convolutional neural network (CNN) is a type of DNN that can be trained to extract important features contained in the input data, which can then be used as inputs to the other hidden layers, significantly improving the performance of the overall network. (D) Some details of the feature extraction process of a CNN, which consists of several hidden layers. First, it has multiple filters (F1, F2, F3), each configured to capture specific features. This process can greatly increase the size of the data, so a pooling layer (P1, P2, P3) is then used to reduce this size. The pooling process does not lead to the loss of valuable data; instead, it helps remove noise and consolidate meaningful data. The flattening layer converts the pooled data into a 1-dimensional stream. This serves as an input for the subsequent fully connected layer, which does the final evaluation to produce the output based on the features extracted by the convolution layers. (E) A CNN with a long short-term memory (LSTM) layer. The additional LSTM layer enables the network to benefit from long-term memory, in addition to the existent short-term working memory. (F) The LSTM layer achieves this long-term memory through its ability to relay both the cell state (dashed green arrows) and the output generated by each module (solid maroon arrows) across its several modules, allowing the flow of useful information. This enables the network to better identify context in the input data over longer time periods. CNN-LSTMs have been found useful for predicting time series data.\n\n\nA faster way to model neuronal circuitry - Scientific Figure on ResearchGate. Available from:\n\n\n\n\n\n\n\n\n\n\nComplete chart of Neural Networks\n\n\nFeedforward Neural Networks (FNNs)\nFeedforward Neural Networks (FNNs) represent the most basic type of neural network architecture. In an FNN, data moves in one direction—from input nodes, through hidden nodes (if present), and finally to output nodes. There are no cycles or loops; thus, information does not flow backward. This structure allows for straightforward processing and is primarily used for tasks where the output is determined solely by the input data without considering previous states[6][1].\nConvolutional Neural Networks (CNNs)\nConvolutional Neural Networks (CNNs) are specialized for processing structured grid data, such as images. They utilize convolutional layers that detect significant features in the input data, enabling the network to classify images or recognize patterns effectively. CNNs consist of multiple layers that perform convolutions and pooling operations to reduce dimensionality while preserving essential information, making them particularly suitable for tasks like image recognition and video analysis[7][8].\nRecurrent Neural Networks (RNNs)\nRecurrent Neural Networks (RNNs) are designed for sequence prediction tasks, where the order of data matters, such as time-series analysis or natural language processing. RNNs have loops that allow information to persist, enabling them to remember previous inputs and utilize this context to inform their output. This architecture is particularly effective for applications involving temporal dependencies and sequential data[4].\nSelf-Organizing Maps (SOMs)\nSelf-Organizing Maps (SOMs) are a type of unsupervised learning neural network that clusters and visualizes complex data. They organize input data into a lower-dimensional representation while preserving the topological properties of the original data space. This method is useful for tasks such as market segmentation and image processing, where hidden patterns and relationships within data are revealed without predefined labels[9].\nRadial Basis Function Networks (RBFNs)\nRadial Basis Function Networks (RBFNs) use radial basis functions as activation functions. They are particularly effective for interpolation in multi-dimensional space. RBFNs can approximate complex functions and are commonly employed in classification tasks and function approximation, serving as a valuable tool for machine learning applications[4].\n\n\n\nThe training of neural networks is a crucial process that enables these models to learn from data and make accurate predictions.\nThis process can be broken down into several key components, including the training methods and the iterative learning cycle.\nTraining Methods: Neural networks are primarily trained using three different approaches:\n\nsupervised learning,\nunsupervised learning,\nand reinforcement learning.\n\nSupervised Learning\nIn supervised learning, the neural network is provided with labeled training data, which includes input-output pairs. This allows the network to learn specific features by comparing its predictions against the known outputs. The goal is to minimize the difference between the predicted outputs and the actual outputs, often referred to as the loss function[10][11].\nUnsupervised Learning\nUnsupervised learning differs from supervised learning in that the neural network works with data that does not have labeled outputs. The primary aim is to discover the underlying structure and patterns within the input data. Techniques such as clustering and association are commonly employed in this approach[12][13].\nReinforcement Learning\nReinforcement learning enables a neural network to learn through interaction with its environment. In this framework, the network receives feedback in the form of rewards or penalties based on its actions, guiding it to develop strategies that maximize cumulative rewards over time. This method is particularly effective in areas like gaming and decision-making tasks[14][15].\n\n\n\n\n\n\n\n\n\nANNs learning process\n\n\nThe learning process of a neural network is iterative and consists of three main phases:\n\nforward propagation,\nloss function calculation,\nand backward propagation.\n\nForward Propagation\nDuring forward propagation, data is passed through the layers of the network, with each neuron processing the inputs based on weighted parameters (weights and biases). The weights determine the significance of each input, while biases influence the activation of the neurons[11][14].\nCalculation of the Loss Function\nAfter forward propagation, the network’s output is evaluated against the desired output using a loss function. This function quantifies the difference between the predicted and actual values, providing a measure of how well the network is performing[11][14].\nBackward Propagation\nBackward propagation, or backpropagation, is the process of adjusting the weights and biases in the network based on the calculated loss. This step involves propagating the error backwards through the network, allowing the model to learn from its mistakes and improve its performance in future predictions. This iterative process continues until the loss is minimized, and the network achieves satisfactory accuracy[11][14].\nBy effectively combining these training methods and processes, neural networks can tackle complex tasks and make reliable predictions across various applications, including computer vision, natural language processing, and financial modeling[16][12].\n\n\n\n\nArtificial Neural Networks (ANNs) have a wide range of applications across various industries, significantly enhancing operational efficiency and decision-making processes. Their versatility allows them to address complex problems in fields such as healthcare, eCommerce, computer vision, and natural language processing:\n\nNatural Language Processing: In the realm of natural language processing (NLP), ANNs contribute significantly to the development of applications such as chatbots and language translation services.\n\nModels like BERT have transformed how machines understand human language, enabling them to grasp context and semantics more effectively. This capability allows for improved customer service interactions and the automation of repetitive tasks, thereby enhancing overall productivity[17][18].\n\nHealthcare: In healthcare, ANNs are utilized to analyze vast amounts of medical data, assisting in diagnosing diseases, predicting patient outcomes, and optimizing treatment plans.\n\nFor instance, neural networks can be employed to identify patterns in patient records that correlate with successful treatment outcomes, ultimately improving the quality of care provided[19].\n\nAdditionally, ANN applications in imaging analysis have shown promise in detecting anomalies in medical images, such as tumors in radiology scans, thereby aiding radiologists in making accurate assessments[19].\n\n\neCommerce: In the eCommerce sector, ANNs play a crucial role in personalizing the shopping experience.\n\nRetail giants like Amazon and AliExpress leverage AI-driven algorithms to recommend products based on user behavior and preferences. By analyzing browsing patterns and purchase history, these platforms can suggest related items, enhancing customer satisfaction and increasing sales[20][21]. Furthermore, ANNs are employed in inventory management and demand forecasting, helping businesses optimize stock levels and reduce costs associated with overstocking or stockouts[20].\n\nComputer Vision: ANNs are foundational to advancements in computer vision, particularly in image recognition tasks. This technology enables applications such as facial recognition and object detection, which have widespread uses in security systems and social media platforms.\n\nFor instance, convolutional neural networks (CNNs) are specifically designed to process and analyze visual data, identifying features and providing accurate descriptions of images[21][22]. As a result, businesses can streamline operations, improve customer engagement, and derive insights from visual data analytics[22].\n\n\n\n\n\nArtificial Neural Networks (ANN) face several challenges and limitations that impact their effectiveness and applicability in various fields, particularly in healthcare and data analysis:\n\nEthical Considerations: Ethical dilemmas also arise with the use of ANNs, particularly regarding issues of bias, accountability, and the implications of automated decision-making in critical areas such as healthcare[23]. Ensuring that ANN systems are fair, transparent, and accountable remains an ongoing concern that researchers and practitioners must address as the technology continues to evolve[23].\nOverfitting and Generalization: One significant challenge associated with ANNs is the phenomenon of overfitting, where the model becomes overly complex and tailored to the training data, failing to generalize to new, unseen data[24].\n\nOverfitting can occur when the model captures not only the underlying patterns but also the noise present in the training dataset.\nRegularization techniques are often employed to mitigate this issue, but finding the right balance between model complexity and generalization remains a critical challenge[24].\n\nInterpretability and Transparency: Another major limitation of ANNs is their poor interpretability.\n\nUnlike traditional models, which can explicitly identify causal relationships, ANNs often function as “black boxes” where the reasoning behind their predictions is unclear[25]. This lack of transparency can hinder trust and acceptance, especially in sensitive applications such as healthcare, where understanding the basis for a diagnosis or treatment recommendation is crucial for both practitioners and patients[25].\n\nTraining Data Requirements: ANNs require large amounts of training data to perform effectively, which can be a significant barrier in scenarios where data availability is limited or where privacy concerns restrict access to sensitive information[25].\n\nAdditionally, the process of training ANNs can be time-consuming and computationally intensive, necessitating substantial computational resources and potentially leading to increased costs[25].\n\nImplementation Challenges: The integration of ANNs into existing workflows and systems also presents numerous challenges. Issues related to interoperability, standardization, and the alignment of ANNs with clinical workflows can impede their successful adoption[25].\n\nFurthermore, the complexity of model development often necessitates multiple iterations, which can be resource-intensive and may deter organizations from fully leveraging ANN capabilities[25].\n\n\n\n\n\n\nThe field of artificial neural networks (ANNs) is rapidly evolving, with ongoing research aimed at enhancing the architecture and functionality of these systems. Future developments are anticipated to focus on biologically inspired designs that emulate the cognitive processes of the human brain. This includes creating architectures capable of both memorization and forgetting, which could lead to more efficient and adaptable learning systems[15].\n\nAs the integration of neuroscience and AI progresses, researchers speculate that the collaboration between bottom-up approaches, which build on neural circuit principles, and top-down methodologies, which leverage insights from cognitive tasks, will illuminate new pathways for creating intelligent machines[17].\nDespite the advancements in ANN technology, several barriers to adoption remain. These include issues related to interoperability, privacy, and the integration of health information technology (HIT) into clinical workflows. Addressing these challenges will require a multi-faceted approach, encompassing technical innovations alongside solutions for political, fiscal, and cultural hurdles[25]. The promotion of standards and controlled terminologies is essential for the broader implementation of AI in healthcare and other sectors[25].\nANNs are already transforming industries such as finance, healthcare, and entertainment, but their potential applications are far from exhausted. As scalability improves, particularly with architectures designed to handle large datasets efficiently, new applications in areas like energy consumption forecasting and predictive analytics are likely to emerge[26]. Furthermore, the exploration of diverse types of neural networks, including feedforward, recurrent, and convolutional networks, will enable customized solutions tailored to specific challenges in various domains[27].\nThe future of ANNs also hinges on community-driven research efforts. Initiatives like Frontiers’ Research Topics encourage collaboration among researchers across disciplines, fostering a vibrant ecosystem for knowledge exchange and innovation[28]. This collaborative spirit is expected to accelerate the pace of discovery and application in the field of AI, opening up new opportunities for exploration and development.\n\nArtificial Neural Networks",
    "crumbs": [
      "What is ANN?",
      "ANN: Introduction"
    ]
  },
  {
    "objectID": "ann/ann-introduction.html#structure-of-artificial-neural-networks",
    "href": "ann/ann-introduction.html#structure-of-artificial-neural-networks",
    "title": "ANN: Introduction",
    "section": "",
    "text": "Artificial Neural Networks (ANNs) are structured to mimic the interconnected network of neurons in the human brain, facilitating complex information processing. The architecture of an ANN typically consists of three main types of layers: the input layer, hidden layers, and the output layer.\n\n\nThe input layer serves as the initial point of data entry for the neural network. It receives information from the external environment in various forms such as numbers, letters, audio files, or image pixels. Each node in the input layer corresponds to a specific feature of the data being processed, and it plays a critical role in transmitting this information to subsequent layers for further analysis[1][2].\n\n\n\nHidden layers are positioned between the input and output layers and are responsible for the majority of the processing within the ANN. A network may have one or multiple hidden layers, each containing numerous artificial neurons. These layers perform complex mathematical operations on the input data, identifying patterns and relationships that are not immediately apparent. The output from one hidden layer becomes the input for the next layer, allowing for a deeper analysis of the data at each stage[1][3][2].\n\n\nHidden layers can vary in their configuration and function. They may employ different types of activation functions, which introduce non-linear properties to the network, enabling it to learn complex mappings from inputs to outputs. The choice of activation function can significantly impact the performance of the neural network in various tasks[4][5].\n\n\n\n\nThe output layer is the final layer in the ANN structure and provides the ultimate result of the computations performed by the network. Depending on the nature of the task, the output layer can contain a single node for binary classification or multiple nodes for multi-class classification problems. Each output node generates a response based on the processed information, translating the network’s findings into actionable insights or predictions[1][3][5].",
    "crumbs": [
      "What is ANN?",
      "ANN: Introduction"
    ]
  },
  {
    "objectID": "ann/ann-introduction.html#types-of-artificial-neural-networks",
    "href": "ann/ann-introduction.html#types-of-artificial-neural-networks",
    "title": "ANN: Introduction",
    "section": "",
    "text": "Artificial Neural Networks (ANNs) can be categorized into several key types, each designed for specific tasks and applications.\n\n\nFeedforward Neural Networks (FNNs) represent the most basic type of neural network architecture. In an FNN, data moves in one direction—from input nodes, through hidden nodes (if present), and finally to output nodes. There are no cycles or loops; thus, information does not flow backward. This structure allows for straightforward processing and is primarily used for tasks where the output is determined solely by the input data without considering previous states[6][1].\n\n\n\nConvolutional Neural Networks (CNNs) are specialized for processing structured grid data, such as images. They utilize convolutional layers that detect significant features in the input data, enabling the network to classify images or recognize patterns effectively. CNNs consist of multiple layers that perform convolutions and pooling operations to reduce dimensionality while preserving essential information, making them particularly suitable for tasks like image recognition and video analysis[7][8].\n\n\n\nRecurrent Neural Networks (RNNs) are designed for sequence prediction tasks, where the order of data matters, such as time-series analysis or natural language processing. RNNs have loops that allow information to persist, enabling them to remember previous inputs and utilize this context to inform their output. This architecture is particularly effective for applications involving temporal dependencies and sequential data[4].\n\n\n\nSelf-Organizing Maps (SOMs) are a type of unsupervised learning neural network that clusters and visualizes complex data. They organize input data into a lower-dimensional representation while preserving the topological properties of the original data space. This method is useful for tasks such as market segmentation and image processing, where hidden patterns and relationships within data are revealed without predefined labels[9].\n\n\n\nRadial Basis Function Networks (RBFNs) use radial basis functions as activation functions. They are particularly effective for interpolation in multi-dimensional space. RBFNs can approximate complex functions and are commonly employed in classification tasks and function approximation, serving as a valuable tool for machine learning applications[4].",
    "crumbs": [
      "What is ANN?",
      "ANN: Introduction"
    ]
  },
  {
    "objectID": "ann/ann-introduction.html#training-of-neural-networks",
    "href": "ann/ann-introduction.html#training-of-neural-networks",
    "title": "ANN: Introduction",
    "section": "",
    "text": "The training of neural networks is a crucial process that enables these models to learn from data and make accurate predictions. This process can be broken down into several key components, including the training methods and the iterative learning cycle.\n\n\nNeural networks are primarily trained using three different approaches: supervised learning, unsupervised learning, and reinforcement learning.\n\n\nIn supervised learning, the neural network is provided with labeled training data, which includes input-output pairs. This allows the network to learn specific features by comparing its predictions against the known outputs. The goal is to minimize the difference between the predicted outputs and the actual outputs, often referred to as the loss function[10][11].\n\n\n\nUnsupervised learning differs from supervised learning in that the neural network works with data that does not have labeled outputs. The primary aim is to discover the underlying structure and patterns within the input data. Techniques such as clustering and association are commonly employed in this approach[12][13].\n\n\n\nReinforcement learning enables a neural network to learn through interaction with its environment. In this framework, the network receives feedback in the form of rewards or penalties based on its actions, guiding it to develop strategies that maximize cumulative rewards over time. This method is particularly effective in areas like gaming and decision-making tasks[14][15].\n\n\n\n\nThe learning process of a neural network is iterative and consists of three main phases: forward propagation, loss function calculation, and backward propagation.\n\n\nDuring forward propagation, data is passed through the layers of the network, with each neuron processing the inputs based on weighted parameters (weights and biases). The weights determine the significance of each input, while biases influence the activation of the neurons[11][14].\n\n\n\nAfter forward propagation, the network’s output is evaluated against the desired output using a loss function. This function quantifies the difference between the predicted and actual values, providing a measure of how well the network is performing[11][14].\n\n\n\nBackward propagation, or backpropagation, is the process of adjusting the weights and biases in the network based on the calculated loss. This step involves propagating the error backwards through the network, allowing the model to learn from its mistakes and improve its performance in future predictions. This iterative process continues until the loss is minimized, and the network achieves satisfactory accuracy[11][14].\nBy effectively combining these training methods and processes, neural networks can tackle complex tasks and make reliable predictions across various applications, including computer vision, natural language processing, and financial modeling[16][12].",
    "crumbs": [
      "What is ANN?",
      "ANN: Introduction"
    ]
  },
  {
    "objectID": "ann/ann-introduction.html#applications",
    "href": "ann/ann-introduction.html#applications",
    "title": "ANN: Introduction",
    "section": "",
    "text": "Artificial Neural Networks (ANNs) have a wide range of applications across various industries, significantly enhancing operational efficiency and decision-making processes. Their versatility allows them to address complex problems in fields such as healthcare, eCommerce, computer vision, and natural language processing:\n\nNatural Language Processing: In the realm of natural language processing (NLP), ANNs contribute significantly to the development of applications such as chatbots and language translation services.\n\nModels like BERT have transformed how machines understand human language, enabling them to grasp context and semantics more effectively. This capability allows for improved customer service interactions and the automation of repetitive tasks, thereby enhancing overall productivity[17][18].\n\nHealthcare: In healthcare, ANNs are utilized to analyze vast amounts of medical data, assisting in diagnosing diseases, predicting patient outcomes, and optimizing treatment plans.\n\nFor instance, neural networks can be employed to identify patterns in patient records that correlate with successful treatment outcomes, ultimately improving the quality of care provided[19].\n\nAdditionally, ANN applications in imaging analysis have shown promise in detecting anomalies in medical images, such as tumors in radiology scans, thereby aiding radiologists in making accurate assessments[19].\n\n\neCommerce: In the eCommerce sector, ANNs play a crucial role in personalizing the shopping experience.\n\nRetail giants like Amazon and AliExpress leverage AI-driven algorithms to recommend products based on user behavior and preferences. By analyzing browsing patterns and purchase history, these platforms can suggest related items, enhancing customer satisfaction and increasing sales[20][21]. Furthermore, ANNs are employed in inventory management and demand forecasting, helping businesses optimize stock levels and reduce costs associated with overstocking or stockouts[20].\n\nComputer Vision: ANNs are foundational to advancements in computer vision, particularly in image recognition tasks. This technology enables applications such as facial recognition and object detection, which have widespread uses in security systems and social media platforms.\n\nFor instance, convolutional neural networks (CNNs) are specifically designed to process and analyze visual data, identifying features and providing accurate descriptions of images[21][22]. As a result, businesses can streamline operations, improve customer engagement, and derive insights from visual data analytics[22].",
    "crumbs": [
      "What is ANN?",
      "ANN: Introduction"
    ]
  },
  {
    "objectID": "ann/ann-introduction.html#challenges-and-limitations",
    "href": "ann/ann-introduction.html#challenges-and-limitations",
    "title": "ANN: Introduction",
    "section": "",
    "text": "Artificial Neural Networks (ANN) face several challenges and limitations that impact their effectiveness and applicability in various fields, particularly in healthcare and data analysis:\n\nEthical Considerations: Ethical dilemmas also arise with the use of ANNs, particularly regarding issues of bias, accountability, and the implications of automated decision-making in critical areas such as healthcare[23]. Ensuring that ANN systems are fair, transparent, and accountable remains an ongoing concern that researchers and practitioners must address as the technology continues to evolve[23].\nOverfitting and Generalization: One significant challenge associated with ANNs is the phenomenon of overfitting, where the model becomes overly complex and tailored to the training data, failing to generalize to new, unseen data[24].\n\nOverfitting can occur when the model captures not only the underlying patterns but also the noise present in the training dataset.\nRegularization techniques are often employed to mitigate this issue, but finding the right balance between model complexity and generalization remains a critical challenge[24].\n\nInterpretability and Transparency: Another major limitation of ANNs is their poor interpretability.\n\nUnlike traditional models, which can explicitly identify causal relationships, ANNs often function as “black boxes” where the reasoning behind their predictions is unclear[25]. This lack of transparency can hinder trust and acceptance, especially in sensitive applications such as healthcare, where understanding the basis for a diagnosis or treatment recommendation is crucial for both practitioners and patients[25].\n\nTraining Data Requirements: ANNs require large amounts of training data to perform effectively, which can be a significant barrier in scenarios where data availability is limited or where privacy concerns restrict access to sensitive information[25].\n\nAdditionally, the process of training ANNs can be time-consuming and computationally intensive, necessitating substantial computational resources and potentially leading to increased costs[25].\n\nImplementation Challenges: The integration of ANNs into existing workflows and systems also presents numerous challenges. Issues related to interoperability, standardization, and the alignment of ANNs with clinical workflows can impede their successful adoption[25].\n\nFurthermore, the complexity of model development often necessitates multiple iterations, which can be resource-intensive and may deter organizations from fully leveraging ANN capabilities[25].",
    "crumbs": [
      "What is ANN?",
      "ANN: Introduction"
    ]
  },
  {
    "objectID": "ann/ann-introduction.html#future-directions",
    "href": "ann/ann-introduction.html#future-directions",
    "title": "ANN: Introduction",
    "section": "",
    "text": "The field of artificial neural networks (ANNs) is rapidly evolving, with ongoing research aimed at enhancing the architecture and functionality of these systems. Future developments are anticipated to focus on biologically inspired designs that emulate the cognitive processes of the human brain. This includes creating architectures capable of both memorization and forgetting, which could lead to more efficient and adaptable learning systems[15].\n\nAs the integration of neuroscience and AI progresses, researchers speculate that the collaboration between bottom-up approaches, which build on neural circuit principles, and top-down methodologies, which leverage insights from cognitive tasks, will illuminate new pathways for creating intelligent machines[17].\nDespite the advancements in ANN technology, several barriers to adoption remain. These include issues related to interoperability, privacy, and the integration of health information technology (HIT) into clinical workflows. Addressing these challenges will require a multi-faceted approach, encompassing technical innovations alongside solutions for political, fiscal, and cultural hurdles[25]. The promotion of standards and controlled terminologies is essential for the broader implementation of AI in healthcare and other sectors[25].\nANNs are already transforming industries such as finance, healthcare, and entertainment, but their potential applications are far from exhausted. As scalability improves, particularly with architectures designed to handle large datasets efficiently, new applications in areas like energy consumption forecasting and predictive analytics are likely to emerge[26]. Furthermore, the exploration of diverse types of neural networks, including feedforward, recurrent, and convolutional networks, will enable customized solutions tailored to specific challenges in various domains[27].\nThe future of ANNs also hinges on community-driven research efforts. Initiatives like Frontiers’ Research Topics encourage collaboration among researchers across disciplines, fostering a vibrant ecosystem for knowledge exchange and innovation[28]. This collaborative spirit is expected to accelerate the pace of discovery and application in the field of AI, opening up new opportunities for exploration and development.\n\nArtificial Neural Networks",
    "crumbs": [
      "What is ANN?",
      "ANN: Introduction"
    ]
  },
  {
    "objectID": "ann/ann-mathematical.html",
    "href": "ann/ann-mathematical.html",
    "title": "ANN: Mathematical Model",
    "section": "",
    "text": "Each neuron computes a weighted sum of its inputs, adds a bias term, and applies an activation function. The output of a neuron can be represented as:\n\\[\ny = f\\left( \\sum_{i=1}^{n} w_i x_i + b \\right)\n\\]\nwhere ( w_i ) are the weights, ( x_i ) are the inputs, ( b ) is the bias, and ( f ) is the activation function.\n\n\n\nFrom: Optical neural networks: progress and challenges\n\n\nNeuron structure and artificial neural network. a Structure of biological neurons. b Mathematical inferring process of artificial neurons in multi-layer perceptron, including the input, weights, summation, activation function, and output. c Multi-layer perceptron artificial neural network\n\n\n\n\n\n\n\n\n\nSimplified model\n\n\n\nA biological neuron consists of three main components: dendrites, soma, and axon[2]. Dendrites receive input signals from other neurons. The soma, or cell body, contains the nucleus and processes information. The axon transmits signals to other neurons through synapses[2][8].\n\nAn artificial neuron in a multi-layer perceptron (MLP) mimics the biological neuron’s function:\n\nInputs: Represented as a vector [1] \\[x = [x_1, x_2, ..., x_n]\\]\nWeights: Each input is associated with a weight [1] \\[w_i\\]\nSummation: The neuron computes a weighted sum of inputs:\n\\[v = \\sum_{i=1}^n w_i x_i + b\\]\nwhere \\[b\\] is the bias term[1].\nActivation Function: The sum is passed through an activation function \\[f\\]\n\\[y = f(v)\\]\nCommon activation functions include sigmoid, hyperbolic tangent, and ReLU[1][10].\nOutput: The result \\[y\\] is the neuron’s output[1].\n\nMulti-layer Perceptron Neural Network\nAn MLP consists of multiple layers of interconnected neurons:\n\nInput Layer: Receives the initial data[3].\nHidden Layers: Process information through weighted connections and activation functions[3].\nOutput Layer: Produces the final result[3].\n\nThe network learns by adjusting weights and biases through backpropagation, minimizing a cost function[4][9]. This process allows the MLP to model complex relationships between inputs and outputs, making it suitable for various machine learning tasks[3][9].\nCitations:\n\nNeural Networks ‐ The Mathematical Model\nThe Structure of the Neuron\nMulti-Layer Perceptron Learning in TensorFlow\nYouTube Video\nNeuron - Wikipedia\nMultilayer Perceptrons in Machine Learning\nTFM Lichtner Bajjaoui Aisha\nOverview of Neuron Structure and Function\nMultilayer Perceptron Definition\nConference Proceedings Paper",
    "crumbs": [
      "What is ANN?",
      "ANN: Mathematical Model"
    ]
  },
  {
    "objectID": "ann/ann-mathematical.html#structure-of-biological-neurons",
    "href": "ann/ann-mathematical.html#structure-of-biological-neurons",
    "title": "ANN: Mathematical Model",
    "section": "",
    "text": "Simplified model\n\n\n\nA biological neuron consists of three main components: dendrites, soma, and axon[2]. Dendrites receive input signals from other neurons. The soma, or cell body, contains the nucleus and processes information. The axon transmits signals to other neurons through synapses[2][8].\n\nAn artificial neuron in a multi-layer perceptron (MLP) mimics the biological neuron’s function:\n\nInputs: Represented as a vector [1] \\[x = [x_1, x_2, ..., x_n]\\]\nWeights: Each input is associated with a weight [1] \\[w_i\\]\nSummation: The neuron computes a weighted sum of inputs:\n\\[v = \\sum_{i=1}^n w_i x_i + b\\]\nwhere \\[b\\] is the bias term[1].\nActivation Function: The sum is passed through an activation function \\[f\\]\n\\[y = f(v)\\]\nCommon activation functions include sigmoid, hyperbolic tangent, and ReLU[1][10].\nOutput: The result \\[y\\] is the neuron’s output[1].\n\nMulti-layer Perceptron Neural Network\nAn MLP consists of multiple layers of interconnected neurons:\n\nInput Layer: Receives the initial data[3].\nHidden Layers: Process information through weighted connections and activation functions[3].\nOutput Layer: Produces the final result[3].\n\nThe network learns by adjusting weights and biases through backpropagation, minimizing a cost function[4][9]. This process allows the MLP to model complex relationships between inputs and outputs, making it suitable for various machine learning tasks[3][9].\nCitations:\n\nNeural Networks ‐ The Mathematical Model\nThe Structure of the Neuron\nMulti-Layer Perceptron Learning in TensorFlow\nYouTube Video\nNeuron - Wikipedia\nMultilayer Perceptrons in Machine Learning\nTFM Lichtner Bajjaoui Aisha\nOverview of Neuron Structure and Function\nMultilayer Perceptron Definition\nConference Proceedings Paper",
    "crumbs": [
      "What is ANN?",
      "ANN: Mathematical Model"
    ]
  },
  {
    "objectID": "ann/ann-mathematical.html#mathematical-model-of-artificial-neurons",
    "href": "ann/ann-mathematical.html#mathematical-model-of-artificial-neurons",
    "title": "ANN: Mathematical Model",
    "section": "",
    "text": "An artificial neuron in a multi-layer perceptron (MLP) mimics the biological neuron’s function:\n\nInputs: Represented as a vector \\[x = [x_1, x_2, ..., x_n]\\][1].\nWeights: Each input is associated with a weight \\[w_i\\][1].\nSummation: The neuron computes a weighted sum of inputs:\n\\[v = \\sum_{i=1}^n w_i x_i + b\\]\nwhere \\[b\\] is the bias term[1].\nActivation Function: The sum is passed through an activation function \\[f\\]:\n\\[y = f(v)\\]\nCommon activation functions include sigmoid, hyperbolic tangent, and ReLU[1][10].\nOutput: The result \\[y\\] is the neuron’s output[1].",
    "crumbs": [
      "What is ANN?",
      "ANN: Mathematical Model"
    ]
  },
  {
    "objectID": "ann/ann-mathematical.html#multi-layer-perceptron-neural-network",
    "href": "ann/ann-mathematical.html#multi-layer-perceptron-neural-network",
    "title": "ANN: Mathematical Model",
    "section": "",
    "text": "An MLP consists of multiple layers of interconnected neurons:\n\nInput Layer: Receives the initial data[3].\nHidden Layers: Process information through weighted connections and activation functions[3].\nOutput Layer: Produces the final result[3].\n\nThe network learns by adjusting weights and biases through backpropagation, minimizing a cost function[4][9]. This process allows the MLP to model complex relationships between inputs and outputs, making it suitable for various machine learning tasks[3][9].\nCitations: [1] https://wiki.eecs.yorku.ca/course_archive/2011-12/F/4403/media/ann-_math_model.pdf [2] https://rotel.pressbooks.pub/biologicalpsychology/chapter/the-structure-of-the-neuron/ [3] https://www.geeksforgeeks.org/multi-layer-perceptron-learning-in-tensorflow/ [4] https://www.youtube.com/watch?v=b7NnMZPNIXA [5] https://en.wikipedia.org/wiki/Neuron [6] https://www.datacamp.com/tutorial/multilayer-perceptrons-in-machine-learning [7] https://diposit.ub.edu/dspace/bitstream/2445/180441/2/tfm_lichtner_bajjaoui_aisha.pdf [8] https://www.khanacademy.org/science/biology/human-biology/neuron-nervous-system/a/overview-of-neuron-structure-and-function [9] https://blog.marketmuse.com/glossary/multilayer-percpetron-definition/ [10] https://www.iitf.lbtu.lv/conference/proceedings2023/Papers/TF007.pdf",
    "crumbs": [
      "What is ANN?",
      "ANN: Mathematical Model"
    ]
  },
  {
    "objectID": "ann/ann-index.html#anns-mathematics",
    "href": "ann/ann-index.html#anns-mathematics",
    "title": "ANN: Table of Contents",
    "section": "",
    "text": "Structure of Biological Neurons",
    "crumbs": [
      "What is ANN?",
      "ANN: Table of Contents"
    ]
  },
  {
    "objectID": "ann/ann-introduction.html#structure-of-anns",
    "href": "ann/ann-introduction.html#structure-of-anns",
    "title": "ANN: Introduction",
    "section": "",
    "text": "Artificial Neural Networks (ANNs) are structured to mimic the interconnected network of neurons in the human brain, facilitating complex information processing.\nThe architecture of an ANN typically consists of three main types of layers: the input layer, hidden layers, and the output layer.\n\nInput Layer\nThe input layer serves as the initial point of data entry for the neural network. It receives information from the external environment in various forms such as numbers, letters, audio files, or image pixels.\nEach node in the input layer corresponds to a specific feature of the data being processed, and it plays a critical role in transmitting this information to subsequent layers for further analysis[1][2].\nHidden Layers\nHidden layers are positioned between the input and output layers and are responsible for the majority of the processing within the ANN. A network may have one or multiple hidden layers, each containing numerous artificial neurons.\nThese layers perform complex mathematical operations on the input data, identifying patterns and relationships that are not immediately apparent. The output from one hidden layer becomes the input for the next layer, allowing for a deeper analysis of the data at each stage[1][3][2].\n\nTypes of Hidden Layers: Hidden layers can vary in their configuration and function. They may employ different types of activation functions, which introduce non-linear properties to the network, enabling it to learn complex mappings from inputs to outputs.\nThe choice of activation function can significantly impact the performance of the neural network in various tasks[4][5].\n\nOutput Layer\nThe output layer is the final layer in the ANN structure and provides the ultimate result of the computations performed by the network. Depending on the nature of the task, the output layer can contain a single node for binary classification or multiple nodes for multi-class classification problems.\nEach output node generates a response based on the processed information, translating the network’s findings into actionable insights or predictions[1][3][5].",
    "crumbs": [
      "What is ANN?",
      "ANN: Introduction"
    ]
  },
  {
    "objectID": "ann/ann-introduction.html#types-of-aneural-networks",
    "href": "ann/ann-introduction.html#types-of-aneural-networks",
    "title": "ANN: Introduction",
    "section": "",
    "text": "Artificial Neural Networks (ANNs) can be categorized into several key types, each designed for specific tasks and applications.\n\n\nFeedforward Neural Networks (FNNs) represent the most basic type of neural network architecture. In an FNN, data moves in one direction—from input nodes, through hidden nodes (if present), and finally to output nodes. There are no cycles or loops; thus, information does not flow backward. This structure allows for straightforward processing and is primarily used for tasks where the output is determined solely by the input data without considering previous states[6][1].\n\n\n\nConvolutional Neural Networks (CNNs) are specialized for processing structured grid data, such as images. They utilize convolutional layers that detect significant features in the input data, enabling the network to classify images or recognize patterns effectively. CNNs consist of multiple layers that perform convolutions and pooling operations to reduce dimensionality while preserving essential information, making them particularly suitable for tasks like image recognition and video analysis[7][8].\n\n\n\nRecurrent Neural Networks (RNNs) are designed for sequence prediction tasks, where the order of data matters, such as time-series analysis or natural language processing. RNNs have loops that allow information to persist, enabling them to remember previous inputs and utilize this context to inform their output. This architecture is particularly effective for applications involving temporal dependencies and sequential data[4].\n\n\n\nSelf-Organizing Maps (SOMs) are a type of unsupervised learning neural network that clusters and visualizes complex data. They organize input data into a lower-dimensional representation while preserving the topological properties of the original data space. This method is useful for tasks such as market segmentation and image processing, where hidden patterns and relationships within data are revealed without predefined labels[9].\n\n\n\nRadial Basis Function Networks (RBFNs) use radial basis functions as activation functions. They are particularly effective for interpolation in multi-dimensional space. RBFNs can approximate complex functions and are commonly employed in classification tasks and function approximation, serving as a valuable tool for machine learning applications[4].",
    "crumbs": [
      "What is ANN?",
      "ANN: Introduction"
    ]
  },
  {
    "objectID": "ann/ann-introduction.html#types-of-annetworks",
    "href": "ann/ann-introduction.html#types-of-annetworks",
    "title": "ANN: Introduction",
    "section": "",
    "text": "Artificial Neural Networks (ANNs) can be categorized into several key types, each designed for specific tasks and applications.\n\n\nFeedforward Neural Networks (FNNs) represent the most basic type of neural network architecture. In an FNN, data moves in one direction—from input nodes, through hidden nodes (if present), and finally to output nodes. There are no cycles or loops; thus, information does not flow backward. This structure allows for straightforward processing and is primarily used for tasks where the output is determined solely by the input data without considering previous states[6][1].\n\n\n\nConvolutional Neural Networks (CNNs) are specialized for processing structured grid data, such as images. They utilize convolutional layers that detect significant features in the input data, enabling the network to classify images or recognize patterns effectively. CNNs consist of multiple layers that perform convolutions and pooling operations to reduce dimensionality while preserving essential information, making them particularly suitable for tasks like image recognition and video analysis[7][8].\n\n\n\nRecurrent Neural Networks (RNNs) are designed for sequence prediction tasks, where the order of data matters, such as time-series analysis or natural language processing. RNNs have loops that allow information to persist, enabling them to remember previous inputs and utilize this context to inform their output. This architecture is particularly effective for applications involving temporal dependencies and sequential data[4].\n\n\n\nSelf-Organizing Maps (SOMs) are a type of unsupervised learning neural network that clusters and visualizes complex data. They organize input data into a lower-dimensional representation while preserving the topological properties of the original data space. This method is useful for tasks such as market segmentation and image processing, where hidden patterns and relationships within data are revealed without predefined labels[9].\n\n\n\nRadial Basis Function Networks (RBFNs) use radial basis functions as activation functions. They are particularly effective for interpolation in multi-dimensional space. RBFNs can approximate complex functions and are commonly employed in classification tasks and function approximation, serving as a valuable tool for machine learning applications[4].",
    "crumbs": [
      "What is ANN?",
      "ANN: Introduction"
    ]
  },
  {
    "objectID": "ann/ann-introduction.html#types-of-anns",
    "href": "ann/ann-introduction.html#types-of-anns",
    "title": "ANN: Introduction",
    "section": "",
    "text": "Artificial Neural Networks (ANNs) can be categorized into several key types, each designed for specific tasks and applications.\n\n\n\n\n\n\n\nFigure 1. Illustration of various types of artificial neural networks (ANN) and their associated components\n\n\n\n\nFigure 1. Illustration of various types of artificial neural networks (ANN) and their associated components\n\n(A) A basic ANN consists of an input layer (red circles), one or more hidden layers (peach circles), and an output layer (blue circle). In the case of neuronal modelling, the input could be features such as the membrane potential (V m ), and the excitatory (exc) and inhibitory (inh) synaptic inputs. The hidden layers perform computations on the inputs, with the actual operations depending on the type of ANN. Their objective is to identify features in the inputs and use these to correlate a given input and the correct output. An ANN can have multiple outputs: in this example, the output is a prediction of the membrane potential. (B) A deep neural network (DNN) is an ANN with multiple hidden layers. (C) A convolutional neural network (CNN) is a type of DNN that can be trained to extract important features contained in the input data, which can then be used as inputs to the other hidden layers, significantly improving the performance of the overall network. (D) Some details of the feature extraction process of a CNN, which consists of several hidden layers. First, it has multiple filters (F1, F2, F3), each configured to capture specific features. This process can greatly increase the size of the data, so a pooling layer (P1, P2, P3) is then used to reduce this size. The pooling process does not lead to the loss of valuable data; instead, it helps remove noise and consolidate meaningful data. The flattening layer converts the pooled data into a 1-dimensional stream. This serves as an input for the subsequent fully connected layer, which does the final evaluation to produce the output based on the features extracted by the convolution layers. (E) A CNN with a long short-term memory (LSTM) layer. The additional LSTM layer enables the network to benefit from long-term memory, in addition to the existent short-term working memory. (F) The LSTM layer achieves this long-term memory through its ability to relay both the cell state (dashed green arrows) and the output generated by each module (solid maroon arrows) across its several modules, allowing the flow of useful information. This enables the network to better identify context in the input data over longer time periods. CNN-LSTMs have been found useful for predicting time series data.\n\n\nA faster way to model neuronal circuitry - Scientific Figure on ResearchGate. Available from:\n\n\n\n\n\n\n\n\n\n\nComplete chart of Neural Networks\n\n\nFeedforward Neural Networks (FNNs)\nFeedforward Neural Networks (FNNs) represent the most basic type of neural network architecture. In an FNN, data moves in one direction—from input nodes, through hidden nodes (if present), and finally to output nodes. There are no cycles or loops; thus, information does not flow backward. This structure allows for straightforward processing and is primarily used for tasks where the output is determined solely by the input data without considering previous states[6][1].\nConvolutional Neural Networks (CNNs)\nConvolutional Neural Networks (CNNs) are specialized for processing structured grid data, such as images. They utilize convolutional layers that detect significant features in the input data, enabling the network to classify images or recognize patterns effectively. CNNs consist of multiple layers that perform convolutions and pooling operations to reduce dimensionality while preserving essential information, making them particularly suitable for tasks like image recognition and video analysis[7][8].\nRecurrent Neural Networks (RNNs)\nRecurrent Neural Networks (RNNs) are designed for sequence prediction tasks, where the order of data matters, such as time-series analysis or natural language processing. RNNs have loops that allow information to persist, enabling them to remember previous inputs and utilize this context to inform their output. This architecture is particularly effective for applications involving temporal dependencies and sequential data[4].\nSelf-Organizing Maps (SOMs)\nSelf-Organizing Maps (SOMs) are a type of unsupervised learning neural network that clusters and visualizes complex data. They organize input data into a lower-dimensional representation while preserving the topological properties of the original data space. This method is useful for tasks such as market segmentation and image processing, where hidden patterns and relationships within data are revealed without predefined labels[9].\nRadial Basis Function Networks (RBFNs)\nRadial Basis Function Networks (RBFNs) use radial basis functions as activation functions. They are particularly effective for interpolation in multi-dimensional space. RBFNs can approximate complex functions and are commonly employed in classification tasks and function approximation, serving as a valuable tool for machine learning applications[4].",
    "crumbs": [
      "What is ANN?",
      "ANN: Introduction"
    ]
  },
  {
    "objectID": "ann/ann-introduction.html#training-of-nns",
    "href": "ann/ann-introduction.html#training-of-nns",
    "title": "ANN: Introduction",
    "section": "",
    "text": "The training of neural networks is a crucial process that enables these models to learn from data and make accurate predictions.\nThis process can be broken down into several key components, including the training methods and the iterative learning cycle.\nTraining Methods: Neural networks are primarily trained using three different approaches:\n\nsupervised learning,\nunsupervised learning,\nand reinforcement learning.\n\nSupervised Learning\nIn supervised learning, the neural network is provided with labeled training data, which includes input-output pairs. This allows the network to learn specific features by comparing its predictions against the known outputs. The goal is to minimize the difference between the predicted outputs and the actual outputs, often referred to as the loss function[10][11].\nUnsupervised Learning\nUnsupervised learning differs from supervised learning in that the neural network works with data that does not have labeled outputs. The primary aim is to discover the underlying structure and patterns within the input data. Techniques such as clustering and association are commonly employed in this approach[12][13].\nReinforcement Learning\nReinforcement learning enables a neural network to learn through interaction with its environment. In this framework, the network receives feedback in the form of rewards or penalties based on its actions, guiding it to develop strategies that maximize cumulative rewards over time. This method is particularly effective in areas like gaming and decision-making tasks[14][15].\n\n\n\n\n\n\n\n\n\nANNs learning process\n\n\nThe learning process of a neural network is iterative and consists of three main phases:\n\nforward propagation,\nloss function calculation,\nand backward propagation.\n\nForward Propagation\nDuring forward propagation, data is passed through the layers of the network, with each neuron processing the inputs based on weighted parameters (weights and biases). The weights determine the significance of each input, while biases influence the activation of the neurons[11][14].\nCalculation of the Loss Function\nAfter forward propagation, the network’s output is evaluated against the desired output using a loss function. This function quantifies the difference between the predicted and actual values, providing a measure of how well the network is performing[11][14].\nBackward Propagation\nBackward propagation, or backpropagation, is the process of adjusting the weights and biases in the network based on the calculated loss. This step involves propagating the error backwards through the network, allowing the model to learn from its mistakes and improve its performance in future predictions. This iterative process continues until the loss is minimized, and the network achieves satisfactory accuracy[11][14].\nBy effectively combining these training methods and processes, neural networks can tackle complex tasks and make reliable predictions across various applications, including computer vision, natural language processing, and financial modeling[16][12].",
    "crumbs": [
      "What is ANN?",
      "ANN: Introduction"
    ]
  },
  {
    "objectID": "deep/deep-index.html#overview",
    "href": "deep/deep-index.html#overview",
    "title": "Deep Learning",
    "section": "",
    "text": "Deep learning is a transformative subset of machine learning that utilizes artificial neural networks (ANNs) to model complex patterns in data, inspired by the human brain’s architecture. Characterized by its ability to process vast amounts of information through multiple interconnected layers, deep learning has driven significant advancements in various fields, including computer vision, natural language processing, and automation.[1][2]\n\n\n\n\n\n\n\nAI fields\n\n\nThe rise of deep learning has enabled machines to achieve performance levels that surpass human capabilities in tasks such as image recognition and language translation, marking a pivotal shift in how artificial intelligence (AI) is applied across industries.[1]\nNotably, deep learning models, particularly deep neural networks (DNNs), have demonstrated remarkable success in diverse applications, such as Convolutional Neural Networks (CNNs) for image analysis and Recurrent Neural Networks (RNNs) for sequential data tasks.[3][4]\nowever, the technology is not without its challenges; issues related to data quality, the demand for skilled professionals, and the interpretability of complex models pose significant hurdles for practitioners and organizations aiming to leverage deep learning effectively.[5][6]\nMoreover, ethical concerns regarding bias and transparency in AI decision-making processes have sparked ongoing debates about the responsible development and deployment of deep learning technologies.[7][8]\nAs industries increasingly rely on deep learning to enhance efficiency and innovation, understanding its foundational principles, capabilities, and limitations has become essential. The architecture of deep learning models involves intricate design choices, including layer composition and training algorithms like backpropagation, which are crucial for optimizing performance.[2][9]\nWith continuous research and development, the field of deep learning is poised for further growth, presenting both opportunities and challenges that must be addressed to harness its full potential in a responsible manner.\ndeep learning is a subset of machine learning that involves algorithms inspired by the structure and function of the brain, known as artificial neural networks (ANNs). ANNs are composed of interconnected units called artificial neurons, which mimic the biological neurons found in the human brain. These networks learn to perform tasks by processing input data through multiple layers of interconnected neurons, where each connection, or synapse, transmits signals to other neurons. The receiving neurons then process these signals and can transmit their output to subsequent neurons, allowing for complex transformations of data[1][2]",
    "crumbs": [
      "Introduction to Deep Learning",
      "Deep Learning"
    ]
  },
  {
    "objectID": "deep/deep-index.html#types-of-deep-learning-models",
    "href": "deep/deep-index.html#types-of-deep-learning-models",
    "title": "Deep Learning",
    "section": "",
    "text": "Deep learning encompasses various types of models designed to address different types of data and tasks. Each model type has its unique architecture and application area, making them suitable for specific use cases.\n\n\n\n\n\n\n\nA comparative view of AI, machine learning, deep learning, and generative AI (source [16]).\n\n\n\nA comparative view of AI, machine learning, deep learning, and generative AI (source [16]).\n\nDeep Neural Networks and Generative AI\nDeep neural networks (DNNs) extend traditional neural networks by adding multiple hidden layers, which allows them to learn more complex patterns within the data. As of 2017, DNNs typically contained thousands to millions of units and connections, enabling them to perform tasks such as image recognition and natural language processing at levels that can surpass human capabilities[1]\nEach layer in a DNN builds on the previous one, allowing the model to capture intricate patterns through its architecture, which can involve convolutional layers for spatial data processing, pooling layers for dimensionality reduction, and fully connected layers for final classification tasks[9][2]\n\nGenerative Adversarial Networks (GANs): Generative Adversarial Networks (GANs) consist of two neural networks: a generator and a discriminator, which are trained simultaneously in a competitive framework.\n\nThe generator aims to create realistic synthetic data, while the discriminator evaluates the authenticity of the generated data against real data samples.\nThis adversarial training process helps improve the quality of generated outputs, making GANs popular in applications such as image generation, video creation, and data augmentation. Their ability to produce high-quality visuals has significantly impacted fields like art, entertainment, and gaming[11][12].\n\nTransformer Models: Transformer models have gained prominence in natural language processing tasks due to their efficiency in handling sequential data without relying on recurrence.\n\nTransformers utilize self-attention mechanisms to weigh the influence of different words in a sentence, allowing for a better understanding of context and relationships within the text.\nThis architecture has led to significant advancements in machine translation, text summarization, and conversational AI applications, making Transformers a crucial development in the deep learning landscape[3].",
    "crumbs": [
      "Introduction to Deep Learning",
      "Deep Learning"
    ]
  }
]