[
  {
    "objectID": "rags/rags-index.html",
    "href": "rags/rags-index.html",
    "title": "RAGs",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n\n\n Back to top",
    "crumbs": [
      "Retrieval-Augmented Generation",
      "RAGs"
    ]
  },
  {
    "objectID": "ann/ann-index.html",
    "href": "ann/ann-index.html",
    "title": "Artificial Neural Networks",
    "section": "",
    "text": "A Brief Introduction to Neural Networks David Kriesel\n\n\n\nA Brief Introduction to Neural Networks David Kriesel\n\n\nFrom Biology to Formalization\nIntroduction, Motivation and History\nHow to teach a computer? You can either write a rigid program – or you can enable the computer to learn on its own. Living beings don’t have any programmer writing a program for developing their skills, which only has to be executed. They learn by themselves – without the initial experience of external knowledge – and thus can solve problems better than any computer today. KaWhat qualities are needed to achieve such a behavior for devices like computers? Can such cognition be adapted from biology? History, development, decline and resurgence of a wide approach to solve problems.\nBiological Neural Networks\nHow do biological systems solve problems? How is a system of neurons working? How can we understand its functionality? What are different quantities of neurons able to do? Where in the nervous system are information processed? A short biological overview of the complexity of simple elements of neural information processing followed by some thoughts about their simplification in order to technically adapt them.\nComponents of Artificial Neural Networks\nFormal definitions and colloquial explanations of the components that realize the technical adaptations of biological neural networks. Initial descriptions of how to combine these components to a neural network.\nHow to Train a Neural Network?\nApproaches and thoughts of how to teach machines. Should neural networks be corrected? Should they only be encouraged? Or should they even learn without any help? Thoughts about what we want to change during the learning procedure and how we will change it, about the measurement of errors and when we have learned enough.\n\n\nSupervised learning Network Paradigms\nThe Perceptron\nA classic among the neural networks. If we talk about a neural network, then in the majority of cases we speak about a percepton or a variation of it. Perceptrons are multi-layer networks without recurrence and with fixed input and output layers. Description of a perceptron, its limits and extensions that should avoid the limitations. Derivation of learning procedures and discussion about their problems.\nRadial Basis Functions\nRBF networks approximate functions by stretching and compressing Gaussians and then summing them spatially shifted. Description of their functions and their learning process. Comparison with multi-layer perceptrons.\nRecurrent Multi-layer Perceptrons\nSome thoughts about networks with internal states. Learning approaches using such networks, overview of their dynamics.\nHopfield Networks\nIn a magnetic field, each particle applies a force to any other particle so that all particles adjust their movements in the energetically most favorable way. This natural mechanism is copied to adjust noisy inputs in order to match their real models.\nLearning Vector Quantisation\nLearning vector quantization is a learning procedure with the aim to reproduce the vector training sets divided in predefined classes as good as possible by using a few representative vectors. If this has been managed, vectors which were unkown until then could easily be assigned to one of these classes.\n\n\nUnsupervised learning Network Paradigms\nSelf Organizing Feature Maps\nA paradigm of unsupervised learning neural networks, which maps an input space by its fixed topology and thus independently looks for simililarities. Function, learning procedure, variations and neural gas.\nAdaptive Resonance Theory\nAn ART network in its original form shall classify binary input vectors, i.e. to assign them to a 1-out-of-n output. Simultaneously, the so far unclassified patterns shall be recognized and assigned to a new class.\n\n\nExcursi, Appendices and Registers\nCluster Analysis and Regional and Online Learnable Fields\nIn Grimm’s dictionary the extinct German word “Kluster” is described by “was dicht und dick zusammensitzet (a thick and dense group of sth.)”. In static cluster analysis, the formation of groups within point clouds is explored. Introduction of some procedures, comparison of their advantages and disadvantages. Discussion of an adaptive clustering method based on neural networks. A regional and online learnable field models from a point cloud, possibly with a lot of points, a comparatively small set of neurons being representative for the point cloud.\nNeural Networks Used for Prediction\nDiscussion of an application of neural networks: A look ahead into the future of time series.\nReinforcement Learning\nWhat if there were no training examples but it would nevertheless be possible to evaluate how good we have learned to solve a problem? et us regard a learning paradigm that is situated between supervised and unsupervised learning.\n\n\n\n\n Back to top",
    "crumbs": [
      "What is ANN?",
      "Artificial Neural Networks"
    ]
  },
  {
    "objectID": "bigdata/bigdata-index.html",
    "href": "bigdata/bigdata-index.html",
    "title": "Big Data",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n\n\n Back to top",
    "crumbs": [
      "What is Big Data?",
      "Big Data"
    ]
  },
  {
    "objectID": "python/python-index.html",
    "href": "python/python-index.html",
    "title": "Python",
    "section": "",
    "text": "Python Programming MOOC 2024\n\n\n\n\nThis is the course material page for the Introduction to Programming course (BSCS1001, 5 ECTS) and the Advanced Course in Programming (BSCS1002, 5 ECTS) from the Department of Computer Science at the University of Helsinki.\n\n\n\nThere are no more live lectures for this year’s MOOC instance. The lecture recordings can be viewed on the table below.\n\nLectures\n\n\n\n\n\nPython Programming MOOC 2024\nAll exercises\nPart 1: Getting started",
    "crumbs": [
      "Python Programming",
      "Python"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Wiki Machine Learning",
    "section": "",
    "text": "Machine learning (ML) is a branch of artificial intelligence (AI) focused on enabling computers and machines to imitate the way that humans learn, to perform tasks autonomously, and to improve their performance and accuracy through experience and exposure to more data.\n\nTransformers are all you need\n\nTransformers (how LLMs work) explained visually | DL5 by 3Blue1Brown\n\nA transformer, functioning as a perceptron within a multilayer large language model (LLM), utilizes self-attention to process input sequences.\nEach word in the sentence “The best Costa Brava spot is …” can attend to all other words, allowing the model to capture contextual relationships and dependencies effectively.\nThis is accomplished through multi-head attention, where multiple attention mechanisms operate in parallel, enabling the model to analyze different aspects of the input simultaneously.\nThe repetition of these processes across layers enhances the model’s ability to refine its predictions, ultimately generating coherent and contextually relevant completions for the sentence.\n\n\n\nMathematical Model of an ANN\nEach neuron computes a weighted sum of its inputs, adds a bias term, and applies an activation function. The output of a neuron can be represented as:\n\\[\ny = f\\left( \\sum_{i=1}^{n} w_i x_i + b \\right)\n\\]\nwhere ( w_i ) are the weights, ( x_i ) are the inputs, ( b ) is the bias, and ( f ) is the activation function.\n\n\n\nFrom: Optical neural networks: progress and challenges\n\n\nNeuron structure and artificial neural network. a Structure of biological neurons. b Mathematical inferring process of artificial neurons in multi-layer perceptron, including the input, weights, summation, activation function, and output. c Multi-layer perceptron artificial neural network\n\n\nPython, Manim and Machine Learning\nPython is a versatile programming language widely used for coding Artificial Neural Networks (ANNs) and Machine Learning (ML) algorithms.\n\nFourier Series\n\nThe Fourier series animation using Manim serves as an excellent example of how Python can be used to create complex visualizations and animations for mathematical concepts.\n\nVideo\nThe Fourier series animation showcases Python’s ability to visualize complex mathematical concepts, which is crucial in ML for understanding data distributions, model architectures, and algorithm behavior.\nSimilarly, when working with ANNs and ML, you would use Python to create visualizations of your model’s architecture, training progress, and prediction results.\nfrom manim import *\n\nclass FourierSeriesAnimation(Scene):\n    def construct(self):\n        # Create axes\n        axes = Axes(\n            x_range=[-2*PI, 2*PI, PI/2],\n            y_range=[-2, 2, 1],\n            axis_config={\"color\": BLUE},\n        )\n        \n        # Create the original function (square wave)\n        def square_wave(x):\n            return np.sign(np.sin(x))\n        \n        original_func = axes.plot(square_wave, color=WHITE)\n        \n        # Define a list of colors for the approximations\n        colors = [RED, GREEN, YELLOW, PURPLE, ORANGE]\n        \n        # Create Fourier series approximations\n        approximations = []\n        for n in range(1, 6):\n            def fourier_series(x):\n                return sum([(4 / ((2*k - 1) * PI)) * np.sin((2*k - 1) * x) for k in range(1, n+1)])\n            \n            approximations.append(axes.plot(fourier_series, color=colors[n-1]))\n        \n        # Add elements to the scene\n        self.add(axes, original_func)\n        \n        # Animate the Fourier series approximations\n        for approx in approximations:\n            self.play(Create(approx), run_time=2)\n            self.wait(1)\n        \n        self.wait(2)\n\n# Render the scene\nif __name__ == \"__main__\":\n    scene = FourierSeriesAnimation()\n    scene.render()\n\n\n\nPython, Quarto and Machine Learning\nQuarto supports executable Python code blocks within markdown.\nThis allows you to create fully reproducible documents and reports—the Python code required to produce your output is part of the document itself, and is automatically re-run whenever the document is rendered.\n\nmatplotlib demo\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis\n\n\n\n\n\n\n\n\nSpiking Neuron Potential\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulation parameters\nT = 100  # Total time in ms\ndt = 1   # Time step in ms\ntime = np.arange(0, T, dt)\n\n# Neuron parameters\nV_th = -50  # Spike threshold in mV\nV_reset = -65  # Reset potential in mV\nR = 1  # Resistance in MΩ\ntau = 10  # Membrane time constant in ms\n\n# Input current (constant for simplicity)\nI = 1.5  # Input current in μA\n\n# Initialize variables\nV_m = V_reset * np.ones(len(time))  # Membrane potential array\nspikes = []  # List to store spike times\n\n# Simulation loop\nfor t in range(1, len(time)):\n    dV = (-(V_m[t-1] - V_reset) + R * I) / tau * dt  # Update membrane potential\n    V_m[t] = V_m[t-1] + dV\n    \n    # Check for spike\n    if V_m[t] &gt;= V_th:\n        spikes.append(t)  # Record spike time\n        V_m[t] = V_reset  # Reset membrane potential after spike\n\n# Plotting results\nplt.figure(figsize=(10, 5))\nplt.plot(time, V_m, label=\"Membrane Potential ($V_m$)\", color='blue')\nplt.plot(spikes, [V_th]*len(spikes), 'ro', label=\"Spikes\")  # Plot spikes as red dots\nplt.axhline(V_th, color='black', linestyle='--', label=\"Spike Threshold\")\nplt.title(\"Membrane Potential of a Spiking Neuron\")\nplt.xlabel(\"Time (ms)\")\nplt.ylabel(\"Membrane Potential (mV)\")\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Membrane Potential of a Spiking Neuron\n\n\n\n\n\n\n\nPerceptron\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Activation function: Sigmoid\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n# Derivative of the sigmoid function\ndef sigmoid_derivative(x):\n    return x * (1 - x)\n\n# Input data (4 samples, 2 features)\nX = np.array([[0, 0],\n              [0, 1],\n              [1, 0],\n              [1, 1]])\n\n# Output data (XOR function)\ny = np.array([[0],\n              [1],\n              [1],\n              [0]])\n\n# Seed for reproducibility\nnp.random.seed(42)\n\n# Initialize weights\ninput_layer_neurons = 2  # Number of input features\nhidden_layer_neurons = 2  # Number of hidden neurons\noutput_neurons = 1        # Number of output neurons\n\n# Weights between input layer and hidden layer\nweights_input_hidden = np.random.uniform(size=(input_layer_neurons, hidden_layer_neurons))\n# Weights between hidden layer and output layer\nweights_hidden_output = np.random.uniform(size=(hidden_layer_neurons, output_neurons))\n\n# Learning rate\nlearning_rate = 0.5\n\n# Training the network\nepochs = 10000\nfor epoch in range(epochs):\n    # Forward pass\n    hidden_layer_activation = np.dot(X, weights_input_hidden)\n    hidden_layer_output = sigmoid(hidden_layer_activation)\n\n    output_layer_activation = np.dot(hidden_layer_output, weights_hidden_output)\n    predicted_output = sigmoid(output_layer_activation)\n\n    # Backpropagation\n    error = y - predicted_output\n    d_predicted_output = error * sigmoid_derivative(predicted_output)\n    \n    error_hidden_layer = d_predicted_output.dot(weights_hidden_output.T)\n    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n\n    # Updating weights\n    weights_hidden_output += hidden_layer_output.T.dot(d_predicted_output) * learning_rate\n    weights_input_hidden += X.T.dot(d_hidden_layer) * learning_rate\n\n# Final predictions after training\nfinal_hidden_layer_activation = np.dot(X, weights_input_hidden)\nfinal_hidden_layer_output = sigmoid(final_hidden_layer_activation)\n\nfinal_output_layer_activation = np.dot(final_hidden_layer_output, weights_hidden_output)\nfinal_predicted_output = sigmoid(final_output_layer_activation)\n\n# Plotting results\nplt.figure(figsize=(10, 5))\nplt.scatter(X[:, 0], X[:, 1], c=final_predicted_output.flatten(), cmap='RdYlBu', s=100)\nplt.title(\"Output of Two-Layer Perceptron (XOR Function)\")\nplt.xlabel(\"Input Feature 1\")\nplt.ylabel(\"Input Feature 2\")\nplt.colorbar(label='Predicted Output')\nplt.grid()\nplt.xlim(-0.5, 1.5)\nplt.ylim(-0.5, 1.5)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Output of Two-Layer Perceptron (XOR Function\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "about/about-roadmap.html",
    "href": "about/about-roadmap.html",
    "title": "Roadmap",
    "section": "",
    "text": "Step by step guide to becoming an AI and Data Scientist in 2024:\n\nAI and Data Scientist\n\nStep by step guide to becoming an AI Engineer in 2024:\n\nAI Engineer\n\n\n\n\n\n\n\n\n\n\n\n\n#\nDate\nTitle\nDescription\nTasks\n\n\n\n\n1\n2024-01-06\nPython Basics 1/4\nIntroduction to Python programming\nVariables, data types\n\n\n1\n2024-01-06\nML Foundations 1/4\nBasic machine learning concepts\nTypes of ML, learning paradigms\n\n\n1\n2024-01-06\nANN Foundation 1/4\nIntroduction to neural networks\nNetwork architecture basics\n\n\n2\n2024-01-13\nPython Basics 2/4\nControl structures\nLoops, conditionals, functions\n\n\n2\n2024-01-13\nML Foundations 2/4\nModel evaluation\nMetrics, validation techniques\n\n\n2\n2024-01-13\nANN Foundation 2/4\nForward propagation\nActivation functions, layers\n\n\n3\n2024-01-20\nPython Basics 3/4\nFunctions and modules\nFunction creation, importing modules\n\n\n3\n2024-01-20\nML Foundations 3/4\nData preprocessing\nCleaning, normalization, feature engineering\n\n\n3\n2024-01-20\nANN Foundation 3/4\nBackpropagation\nGradient descent basics\n\n\n4\n2024-01-27\nPython Basics 4/4\nError handling\nTry-except, debugging\n\n\n4\n2024-01-27\nML Foundations 4/4\nModel selection\nCross-validation, hyperparameter tuning\n\n\n4\n2024-01-27\nANN Foundation 4/4\nLoss functions\nCommon losses, optimization basics\n\n\n5\n2024-02-03\nPython Intermediate 1/4\nOOP concepts\nClasses, objects, inheritance\n\n\n5\n2024-02-03\nSupervised Learning 1/4\nLinear regression\nSimple and multiple regression\n\n\n5\n2024-02-03\nANN Training 1/4\nOptimization algorithms\nSGD, Adam, RMSprop\n\n\n6\n2024-02-10\nPython Intermediate 2/4\nAdvanced functions\nLambda, decorators, generators\n\n\n6\n2024-02-10\nSupervised Learning 2/4\nClassification basics\nLogistic regression, decision trees\n\n\n6\n2024-02-10\nANN Training 2/4\nRegularization\nL1, L2, dropout\n\n\n7\n2024-02-17\nPython Intermediate 3/4\nFile handling\nReading/writing files, JSON\n\n\n7\n2024-02-17\nSupervised Learning 3/4\nEnsemble methods\nRandom forests, boosting\n\n\n7\n2024-02-17\nANN Training 3/4\nBatch normalization\nImplementation and effects\n\n\n8\n2024-02-24\nPython Intermediate 4/4\nPackage management\nPip, virtual environments\n\n\n8\n2024-02-24\nSupervised Learning 4/4\nModel deployment\nSaving, loading, serving models\n\n\n8\n2024-02-24\nANN Training 4/4\nModel evaluation\nMetrics, validation strategies\n\n\n9\n2024-03-02\nPython for DS 1/4\nNumPy basics\nArrays, operations, indexing\n\n\n9\n2024-03-02\nDeep Learning 1/4\nDL frameworks\nPyTorch basics\n\n\n9\n2024-03-02\nANN Advanced 1/4\nAdvanced architectures\nResNet, Inception\n\n\n10\n2024-03-09\nPython for DS 2/4\nPandas basics\nDataFrames, series, indexing\n\n\n10\n2024-03-09\nDeep Learning 2/4\nData loading\nDatasets, dataloaders\n\n\n10\n2024-03-09\nANN Advanced 2/4\nTransfer learning\nPre-trained models, fine-tuning\n\n\n11\n2024-03-16\nPython for DS 3/4\nData visualization\nMatplotlib, Seaborn\n\n\n11\n2024-03-16\nDeep Learning 3/4\nCustom layers\nBuilding network components\n\n\n11\n2024-03-16\nANN Advanced 3/4\nAdvanced training\nLearning rate scheduling\n\n\n12\n2024-03-23\nPython for DS 4/4\nData analysis\nEDA, statistical analysis\n\n\n12\n2024-03-23\nDeep Learning 4/4\nModel optimization\nPerformance tuning\n\n\n12\n2024-03-23\nANN Advanced 4/4\nDeployment\nProduction considerations\n\n\n13\n2024-03-30\nLinear Algebra 1/4\nVectors and matrices\nBasic operations\n\n\n13\n2024-03-30\nCNN 1/4\nCNN basics\nConvolution operations\n\n\n13\n2024-03-30\nANN Applications 1/4\nImage classification\nMNIST implementation\n\n\n14\n2024-04-06\nLinear Algebra 2/4\nMatrix operations\nMultiplication, inverse\n\n\n14\n2024-04-06\nCNN 2/4\nPooling layers\nMax pooling, average pooling\n\n\n14\n2024-04-06\nANN Applications 2/4\nObject detection\nYOLO implementation\n\n\n15\n2024-04-13\nLinear Algebra 3/4\nEigenvalues\nDecomposition\n\n\n15\n2024-04-13\nCNN 3/4\nModern architectures\nVGG, ResNet\n\n\n15\n2024-04-13\nANN Applications 3/4\nSegmentation\nU-Net implementation\n\n\n16\n2024-04-20\nLinear Algebra 4/4\nPCA\nDimensionality reduction\n\n\n16\n2024-04-20\nCNN 4/4\nTransfer learning\nFine-tuning CNNs\n\n\n16\n2024-04-20\nANN Applications 4/4\nStyle transfer\nNeural style transfer\n\n\n17\n2024-04-27\nStatistics 1/4\nProbability basics\nDistributions\n\n\n17\n2024-04-27\nRNN 1/4\nRNN basics\nSequential data\n\n\n17\n2024-04-27\nRAG Basics 1/4\nVector databases\nEmbedding basics\n\n\n18\n2024-05-04\nStatistics 2/4\nHypothesis testing\nT-tests, chi-square\n\n\n18\n2024-05-04\nRNN 2/4\nLSTM\nLong-term dependencies\n\n\n18\n2024-05-04\nRAG Basics 2/4\nRetrieval methods\nVector similarity\n\n\n19\n2024-05-11\nStatistics 3/4\nRegression analysis\nStatistical modeling\n\n\n19\n2024-05-11\nRNN 3/4\nGRU\nGated recurrent units\n\n\n19\n2024-05-11\nRAG Basics 3/4\nQuery processing\nEmbedding generation\n\n\n20\n2024-05-18\nStatistics 4/4\nBayesian stats\nProbabilistic modeling\n\n\n20\n2024-05-18\nRNN 4/4\nAttention mechanism\nSelf-attention basics\n\n\n20\n2024-05-18\nRAG Basics 4/4\nResponse generation\nCombining retrieved info\n\n\n21\n2024-05-25\nBig Data 1/4\nDistributed systems\nHadoop ecosystem\n\n\n21\n2024-05-25\nLab: Classification 1/4\nProject setup\nData preparation\n\n\n21\n2024-05-25\nRAG Implementation 1/4\nSystem design\nArchitecture planning\n\n\n22\n2024-06-01\nBig Data 2/4\nMapReduce\nProgramming paradigm\n\n\n22\n2024-06-01\nLab: Classification 2/4\nModel development\nTraining pipeline\n\n\n22\n2024-06-01\nRAG Implementation 2/4\nIntegration\nConnecting components\n\n\n23\n2024-06-08\nBig Data 3/4\nSpark basics\nRDD operations\n\n\n23\n2024-06-08\nLab: Classification 3/4\nModel evaluation\nPerformance analysis\n\n\n23\n2024-06-08\nRAG Implementation 3/4\nOptimization\nPerformance tuning\n\n\n24\n2024-06-15\nBig Data 4/4\nSpark ML\nML pipelines\n\n\n24\n2024-06-15\nLab: Classification 4/4\nDeployment\nProduction readiness\n\n\n24\n2024-06-15\nRAG Implementation 4/4\nDeployment\nSystem deployment\n\n\n25\n2024-06-22\nLab: NLP 1/4\nText preprocessing\nTokenization, cleaning\n\n\n25\n2024-06-22\nLab: CV 1/4\nImage preprocessing\nAugmentation pipeline\n\n\n25\n2024-06-22\nLab: Full Stack 1/4\nFrontend design\nUI/UX planning\n\n\n26\n2024-06-29\nLab: NLP 2/4\nModel architecture\nBERT implementation\n\n\n26\n2024-06-29\nLab: CV 2/4\nModel development\nCNN architecture\n\n\n26\n2024-06-29\nLab: Full Stack 2/4\nBackend setup\nAPI development\n\n\n27\n2024-07-06\nLab: NLP 3/4\nFine-tuning\nTransfer learning\n\n\n27\n2024-07-06\nLab: CV 3/4\nTraining\nModel optimization\n\n\n27\n2024-07-06\nLab: Full Stack 3/4\nIntegration\nAPI endpoints\n\n\n28\n2024-07-13\nLab: NLP 4/4\nDeployment\nProduction system\n\n\n28\n2024-07-13\nLab: CV 4/4\nDeployment\nModel serving\n\n\n28\n2024-07-13\nLab: Full Stack 4/4\nDeployment\nSystem launch\n\n\n29\n2024-07-20\nFinal Project 1/4\nProject planning\nRequirements analysis\n\n\n29\n2024-07-20\nFinal Project 2/4\nImplementation\nCore development\n\n\n29\n2024-07-20\nFinal Project 3/4\nTesting\nSystem validation\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "About",
      "Roadmap"
    ]
  },
  {
    "objectID": "books/books-index.html",
    "href": "books/books-index.html",
    "title": "Books",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n\n\n Back to top",
    "crumbs": [
      "Book Resources",
      "Books"
    ]
  },
  {
    "objectID": "about/about-index.html",
    "href": "about/about-index.html",
    "title": "About",
    "section": "",
    "text": "Complementary to albertprofe.dev\n\n\n\n Back to top",
    "crumbs": [
      "About",
      "About"
    ]
  },
  {
    "objectID": "labs/labs-index.html",
    "href": "labs/labs-index.html",
    "title": "Labs",
    "section": "",
    "text": "Lab#ANN001: Building ANN recognizes numbers\n\nBuilding a neural network FROM SCRATCH (no Tensorflow/Pytorch, just numpy & math)\nKaggle notebook with all the code\nBlog article with more/clearer math explanation\n\n\n\nLab#GPT001: Python extractor\nAI reads books: Page-by-Page PDF Knowledge Extractor & Summarizer\n\nThe read_books.py script performs an intelligent page-by-page analysis of PDF books, methodically extracting knowledge points and generating progressive summaries at specified intervals. It processes each page individually, allowing for detailed content understanding while maintaining the contextual flow of the book.\n\n\nPython extractor\n\n\n\nLab#ANN002: Tensor for back propagation\nA small auto-grad engine (inspired from Karpathy's micrograd and PyTorch) using Apple's MLX and Numpy. That’s why the name: smolgrad\n\nIt will help y’all understand the core concepts behind automatic differentiation and backpropagation. I mean, AI is literally powered by these things.\n\nwhat is autograd?\nAuto-grad, short for automatic differentiation, is a technique used in machine learning to efficiently compute gradients of complex functions. In the context of neural networks, autograd enables the computation of gradients with respect to the model’s parameters, which is crucial for training the network using optimization algorithms like gradient descent.\n\nIt works by constructing a computational graph that represents the flow of data through the network. Each node in the graph represents a mathematical operation, and the edges represent the flow of data between these operations. By tracking the operations and their dependencies, autograd can automatically compute the gradients using the chain rule of calculus.\n\nAn algorithm used in conjuction with autograd is Backpropagation to train neural networks. It is the process of propagating the gradients backward through the computational graph, from the output layer to the input layer, in order to update the model’s parameters. This is the stuff which makes ML/DL models “learn”.\n\nautograd: repo\n\n\n\nLab#RAG001: Hedge fund trading team\nThis lab outlines the creation of a hedge fund trading team using AI agents. The process involves setting up multiple specialized agents to handle different aspects of trading:\n\nA Market Data Agent to gather and process financial data\nA Quant Agent for developing trading strategies\nA Risk Management Agent to assess and mitigate risks\nA Portfolio Management Agent for overall fund management\n\nThese agents are then integrated into an Agent Graph, allowing them to collaborate and share information efficiently.\n\nThe lab also covers obtaining stock price data and generating trading signals, which are crucial for making informed investment decisions.\n\nA key component of the lab is the creation of a Backtester. This tool allows the team to simulate and evaluate their trading strategies using historical data, providing valuable insights into potential performance without risking real capital.\nThe final step involves running the backtest to assess the effectiveness of the developed strategies and the overall performance of the AI-driven hedge fund team. This approach combines advanced AI techniques with traditional financial analysis, potentially offering a powerful new paradigm for hedge fund management.\n\nSyllabus\n\nSetup\nCreate Market Data Agent\nCreate Quant Agent\nCreate Risk Management Agent\nCreate Portfolio Management Agent\nCreate Agent Graph\nGet Stock Price and Trading Signals\nCreate Backtester\nRun the Backtest\n\n\n\nCitations\n\nHow Hedge Fund Operations Are Organized\nI Gave an AI Hedge Fund $1000 to invest with\nHedge Fund Strategies Introduction\nHedge Fund Trading Team Gist\nHedge Fund Structure\nHedge Fund Performance Evaluation Using Data Envelopment Analysis\nHow to Set Up a Hedge Fund\nFour dimensions of risk management for hedge funds\nhedge-fund-trading-team.ipynb\n\n\n\n\nLab#MAL001: How to structure your ML code\nML apps, like any other piece of software, can only generate business value once they are deployed and used in a production environment.\nAnd the thing is, deploying all-in-one messy Jupyter notebooks from your local machine to a production environment is neither easy, nor recommended from an MLOps perspective.\nOften a DevOps or MLOps senior colleague needs to re-write your all-in-on messy notebook, which adds excessive friction and frustration for you and the guy helping you.\nSo the question is:\n\nIs there a better way to develop and package your ML code, so you ship faster and better?\n\nYes, there is\n\nHow to structure your ML code\n\n\n\nLab#MAL002: How to structure your ML code\nThis is the first article in the series where we will build AI Agents from scratch without using any LLM orchestration frameworks. In this one you will learn:\n\nWhat are agents?\nHow the Tool usage actually works.\nHow to build a decorator wrapper that extracts relevant details from a Python function to be passed to the LLM via system prompt.\nHow to think about constructing effective system prompts that can be used for Agents.\nHow to build an Agent class that is able to plan and execute actions using provided Tools.\n\nYou can find the code examples for this and following projects in this repo\n\nBuilding AI Agents from scratch - Part 1: Tool use\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Lab Exercises",
      "Labs"
    ]
  },
  {
    "objectID": "resources/resources-index.html",
    "href": "resources/resources-index.html",
    "title": "Resources",
    "section": "",
    "text": "Math and Physics\n\nPhysics and Deep Learning\n\nbook Welcome to the Physics-based Deep Learning Book (v0.2)\nRob J Hyndman | Professor of Statistics\n\n\n\nAlgorithms\nMIT’s Introduction to Algorithms:\n\nLecture Notes\nLecture Videos\n\n\n\n\nIntroduction\n\nNeural Network\n\nA Brief Introduction to Neural Networks\nMaking neural nets uncool again\n3blue1brown github videos\n3blue1brown\nrepo manim\nBut what is a neural network? | Deep learning chapter 1\n\n\n\nDeep Learning\n\nMIT Introduction to Deep Learning | 6.S191\nPractical Deep Learning\nMIT 6.S191 Introduction to Deep Learning 2025\nMIT 6.S191 Introduction to Deep Learning 2024\n\n\n\nGenerative AI\n\nGenerative AI in a Nutshell - how to survive and thrive in the age of AI\nGenerative AI in a Nutshell: poster\n\n\n\nMachine Learning\n\nyoutube Learn Machine Learning Like a GENIUS and Not Waste Time\n6.867 Machine Learning (Fall 2004)\n6.867 | Fall 2006 | GraduateMachine Learning\nMachine Learning Resources\n\n\n\n\nTools, papers and labs\n\npaper Attention Is All You Need\nweb IT-Tools\nyoutube Building a neural network FROM SCRATCH (no Tensorflow/Pytorch, just numpy & math)\nAI by hand\npaper Deep learning Yann LeCun1,2, Yoshua Bengio3 & Geoffrey Hinton4,\n\n\n\nWorkflows and automation\n\nn8n\nBuild LLM Apps Easily\nAbacus\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Additional Resources",
      "Resources"
    ]
  },
  {
    "objectID": "algorithms/algo-index.html",
    "href": "algorithms/algo-index.html",
    "title": "Algorithms",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n\n\n Back to top",
    "crumbs": [
      "What is Algorithms?",
      "Algorithms"
    ]
  },
  {
    "objectID": "deep/deep-index.html",
    "href": "deep/deep-index.html",
    "title": "Deep Learning",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n\n\n Back to top",
    "crumbs": [
      "Introduction to Deep Learning",
      "Deep Learning"
    ]
  },
  {
    "objectID": "machine/machine-index.html",
    "href": "machine/machine-index.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Introduction\nMachine learning (ML) is a branch of artificial intelligence (AI) focused on enabling computers and machines to imitate the way that humans learn, to perform tasks autonomously, and to improve their performance and accuracy through experience and exposure to more data.\nUC Berkeley breaks out the learning system of a machine learning algorithm into three main parts.\n\nA Decision Process: In general, machine learning algorithms are used to make a prediction or classification. Based on some input data, which can be labeled or unlabeled, your algorithm will produce an estimate about a pattern in the data.\nAn Error Function: An error function evaluates the prediction of the model. If there are known examples, an error function can make a comparison to assess the accuracy of the model.\nA Model Optimization Process: If the model can fit better to the data points in the training set, then weights are adjusted to reduce the discrepancy between the known example and the model estimate. The algorithm will repeat this iterative “evaluate and optimize” process, updating weights autonomously until a threshold of accuracy has been met.\nWhat is machine learning?\n\n\nMachine learning is a subfield of artificial intelligence, which is broadly defined as the capability of a machine to imitate intelligent human behavior. Artificial intelligence systems are used to perform complex tasks in a way that is similar to how humans solve problems.\n\n\n\nDeep Learning and Machine Learning\n\n\n\nFrom: Optical neural networks: progress and challenges\n\n\nNeuron structure and artificial neural network. a Structure of biological neurons. b Mathematical inferring process of artificial neurons in multi-layer perceptron, including the input, weights, summation, activation function, and output. c Multi-layer perceptron artificial neural network\n\n\nMachine Learning fields\nThe core idea that classifies Deep Learning as a field of Machine Learning (ML) lies in its use of artificial neural networks (ANNs) with multiple layers to automatically learn patterns and representations from data. Unlike traditional ML, which often requires manual feature engineering, deep learning models extract features hierarchically, enabling them to handle complex tasks such as image recognition and natural language processing.\nKey fields in Machine Learning:\n\nSupervised Learning\nUnsupervised Learning\nSemi-Supervised Learning\nReinforcement Learning\n\nSupervised Learning, Unsupervised Learning, Semi-Supervised Learning, and Reinforcement Learning are key paradigms in machine learning. Each approach addresses different types of problems and data availability.\nSupervised learning uses labeled data for training, while unsupervised learning identifies patterns in unlabeled data. Semi-supervised learning combines both, and reinforcement learning focuses on decision-making through trial and error in dynamic environments.\n\n\n\n\n\n\n\n\n\n\n\nSubfield\nDefinition\nObjective\nExamples\nApplications\n\n\n\n\nSupervised Learning\nTrains a model on a labeled dataset where each training example is paired with an output label.\nLearn a mapping from inputs to outputs for predictions on unseen data.\nLinear regression, logistic regression, decision trees, SVMs\nClassification (e.g., spam detection), regression (e.g., house prices)\n\n\nUnsupervised Learning\nTrains a model on data without labeled outputs to learn the underlying structure or distribution of the data.\nIdentify patterns, groupings, or relationships within the data.\nClustering (e.g., k-means, hierarchical), dimensionality reduction (e.g., PCA)\nMarket segmentation, anomaly detection, exploratory data analysis\n\n\nSemi-Supervised Learning\nCombines both labeled and unlabeled data for training, typically with a small amount of labeled data.\nImprove learning accuracy by leveraging additional unlabeled data while benefiting from labeled examples.\nTechniques using supervised methods enhanced by clustering.\nScenarios where labeling data is expensive or time-consuming (e.g., image classification)\n\n\nReinforcement Learning\nInvolves training an agent to make decisions by interacting with an environment and learning through trial and error.\nLearn a policy that maximizes cumulative rewards over time.\nQ-learning, deep reinforcement learning (using neural networks)\nRobotics, game playing (e.g., AlphaGo), autonomous systems\n\n\n\n\n\n\nDeep Learning as distinct field\nDeep learning is indeed considered a distinct field within the broader domain of machine learning.\nIt is characterized by its use of deep neural networks to learn complex patterns from large datasets.\n\n\n\n\n\n\nDeep Learning as a Distinct Field\n\n\n\nCore Concept: Deep learning utilizes artificial neural networks with multiple layers (deep architectures) to automatically learn representations from data, distinguishing it from traditional machine learning methods that often rely on manual feature extraction.\nComplexity Handling: Deep learning excels at managing high-dimensional data and capturing non-linear relationships, making it particularly effective for tasks like image and speech recognition.\n\n\n\nRelationship to Other Machine Learning Subfields\nDeep learning can be viewed as a specialized approach within the following main subfields of machine learning, here’s how deep learning fits into the classification of machine learning and its relationship with other fields:\n\nSupervised Learning:\n\nDefinition: Involves training models on labeled datasets where the output is known.\nDeep Learning Application: Deep learning models, such as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), are frequently used for supervised tasks like classification and regression.\n\nUnsupervised Learning:\n\nDefinition: Involves training models on unlabeled data to discover hidden patterns or structures.\nDeep Learning Application: Techniques like autoencoders and generative adversarial networks (GANs) are used in deep learning for tasks such as clustering and anomaly detection.\n\nSemi-Supervised Learning:\n\nDefinition: Combines both labeled and unlabeled data for training, leveraging the strengths of both approaches.\nDeep Learning Application: Deep learning can enhance semi-supervised methods by using pre-trained models to extract features from unlabeled data.\n\nReinforcement Learning:\n\nDefinition: Involves training agents to make decisions by interacting with an environment, receiving feedback in the form of rewards or penalties.\nDeep Learning Application: Deep reinforcement learning combines neural networks with reinforcement learning principles, enabling agents to learn complex behaviors in environments with high-dimensional state spaces.\n\n\n\nDeep learning represents a significant advancement in machine learning, offering powerful tools for both supervised and unsupervised tasks. Its ability to automatically learn features from raw data sets it apart from traditional machine learning methods, making it a critical area of research and application in modern AI systems.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Machine Learning Basics",
      "Machine Learning"
    ]
  },
  {
    "objectID": "resources/resources-books.html",
    "href": "resources/resources-books.html",
    "title": "Books",
    "section": "",
    "text": "book Welcome to the Physics-based Deep Learning Book (v0.2)\nLLM Engineer’s Handbook: Master the art of engineering large language models from concept to production\nRepo\nThe Little Book of Deep Learning\nUnderstand Deep Learning: Python notebooks covering the whole text\nDeep Learning An MIT Press book Ian Goodfellow and Yoshua Bengio and Aaron Courville\n\n\n\n\n Back to top",
    "crumbs": [
      "Books",
      "Books"
    ]
  },
  {
    "objectID": "python/python-index.html#about-this-course",
    "href": "python/python-index.html#about-this-course",
    "title": "Python",
    "section": "",
    "text": "This is the course material page for the Introduction to Programming course (BSCS1001, 5 ECTS) and the Advanced Course in Programming (BSCS1002, 5 ECTS) from the Department of Computer Science at the University of Helsinki.",
    "crumbs": [
      "Python Programming",
      "Python"
    ]
  },
  {
    "objectID": "python/python-index.html#lectures",
    "href": "python/python-index.html#lectures",
    "title": "Python",
    "section": "",
    "text": "There are no more live lectures for this year’s MOOC instance. The lecture recordings can be viewed on the table below.\n\nLectures",
    "crumbs": [
      "Python Programming",
      "Python"
    ]
  },
  {
    "objectID": "python/python-index.html#links",
    "href": "python/python-index.html#links",
    "title": "Python",
    "section": "",
    "text": "Python Programming MOOC 2024\nAll exercises\nPart 1: Getting started",
    "crumbs": [
      "Python Programming",
      "Python"
    ]
  },
  {
    "objectID": "python/python-part1.html",
    "href": "python/python-part1.html",
    "title": "Python Helsinki: 1",
    "section": "",
    "text": "Getting started\n- Part 1: Getting started\n\n\n\n\n Back to top",
    "crumbs": [
      "Python Programming",
      "Python Helsinki: 1"
    ]
  },
  {
    "objectID": "python/python-part2.html",
    "href": "python/python-part2.html",
    "title": "Python Helsinki: 2",
    "section": "",
    "text": "Programming terminology\n- Part 2: Programming terminology\n\n\n\n\n Back to top",
    "crumbs": [
      "Python Programming",
      "Python Helsinki: 2"
    ]
  },
  {
    "objectID": "python/python-part3.html",
    "href": "python/python-part3.html",
    "title": "Python Helsinki: 3",
    "section": "",
    "text": "More about variables\n- Part 3: More about variables\n\n\n\n\n Back to top",
    "crumbs": [
      "Python Programming",
      "Python Helsinki: 3"
    ]
  },
  {
    "objectID": "python/python-part4.html",
    "href": "python/python-part4.html",
    "title": "Python Helsinki: 4",
    "section": "",
    "text": "Functions\n- Part 4: Functions\n\n\n\n\n Back to top",
    "crumbs": [
      "Python Programming",
      "Python Helsinki: 4"
    ]
  },
  {
    "objectID": "python/python-part5.html",
    "href": "python/python-part5.html",
    "title": "Python Helsinki: 5",
    "section": "",
    "text": "Lists\n- Part 5: Lists\n\n\n\n\n Back to top",
    "crumbs": [
      "Python Programming",
      "Python Helsinki: 5"
    ]
  },
  {
    "objectID": "python/python-part6.html",
    "href": "python/python-part6.html",
    "title": "Python Helsinki: 6",
    "section": "",
    "text": "File I/O and Strings\n- Part 6: File I/O and Strings\n\n\n\n\n Back to top",
    "crumbs": [
      "Python Programming",
      "Python Helsinki: 6"
    ]
  },
  {
    "objectID": "python/python-part7.html",
    "href": "python/python-part7.html",
    "title": "Python Helsinki: 7",
    "section": "",
    "text": "Tuples\n- Part 7: Tuples\n\n\n\n\n Back to top",
    "crumbs": [
      "Python Programming",
      "Python Helsinki: 7"
    ]
  },
  {
    "objectID": "python/python-part8.html",
    "href": "python/python-part8.html",
    "title": "Python Helsinki: 8",
    "section": "",
    "text": "Dictionaries\n- Part 8: Dictionaries\n\n\n\n\n Back to top",
    "crumbs": [
      "Python Programming",
      "Python Helsinki: 8"
    ]
  },
  {
    "objectID": "python/python-part9.html",
    "href": "python/python-part9.html",
    "title": "Python Helsinki: 9",
    "section": "",
    "text": "Objects and Classes\n- Part 9: Objects and Classes\n\n\n\n\n Back to top",
    "crumbs": [
      "Python Programming",
      "Python Helsinki: 9"
    ]
  },
  {
    "objectID": "python/python-part10.html",
    "href": "python/python-part10.html",
    "title": "Python Helsinki: 10",
    "section": "",
    "text": "Object-Oriented Programming\n- Part 10: Object-Oriented Programming\n\n\n\n\n Back to top",
    "crumbs": [
      "Python Programming",
      "Python Helsinki: 10"
    ]
  },
  {
    "objectID": "python/python-part11.html",
    "href": "python/python-part11.html",
    "title": "Python Helsinki: 11",
    "section": "",
    "text": "Advanced Object-Oriented Programming\n- Part 11: Advanced Object-Oriented Programming\n\n\n\n\n Back to top",
    "crumbs": [
      "Python Programming",
      "Python Helsinki: 11"
    ]
  },
  {
    "objectID": "python/python-part12.html",
    "href": "python/python-part12.html",
    "title": "Python Helsinki: 12",
    "section": "",
    "text": "Libraries and Modules\n- Part 12: Libraries and Modules\n\n\n\n\n Back to top",
    "crumbs": [
      "Python Programming",
      "Python Helsinki: 12"
    ]
  },
  {
    "objectID": "python/python-part13.html",
    "href": "python/python-part13.html",
    "title": "Python Helsinki: 13",
    "section": "",
    "text": "Data Processing\n- Part 13: Data Processing\n\n\n\n\n Back to top",
    "crumbs": [
      "Python Programming",
      "Python Helsinki: 13"
    ]
  },
  {
    "objectID": "python/python-part14.html",
    "href": "python/python-part14.html",
    "title": "Python Helsinki: 14",
    "section": "",
    "text": "GUI Programming\n- Part 14: GUI Programming\n\n\n\n\n Back to top",
    "crumbs": [
      "Python Programming",
      "Python Helsinki: 14"
    ]
  },
  {
    "objectID": "generative/generative-transformer.html",
    "href": "generative/generative-transformer.html",
    "title": "Generative AI Transformer",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n\n\n Back to top",
    "crumbs": [
      "Generative AI Transformer",
      "Generative AI Transformer"
    ]
  },
  {
    "objectID": "generative/generative-gpt.html",
    "href": "generative/generative-gpt.html",
    "title": "Generative AI GPT",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n\n\n Back to top",
    "crumbs": [
      "Generative LLM",
      "Generative AI GPT"
    ]
  },
  {
    "objectID": "generative/generative-index.html",
    "href": "generative/generative-index.html",
    "title": "Generative AI",
    "section": "",
    "text": "Generative AI Models\n\nGenerative AI models are machine learning models that can generate new data from existing data.\n\nGenerative AI models are used in many applications, such as image generation, text generation, speech synthesis, and music generation.\nTable with key technologies and models that play crucial roles in Generative AI:\n\n\n\n\n\n\n\n\n\n\n\nTechnology\nYear Released\nDescription\nKey Differences\nApplications\n\n\n\n\nRetrieval-Augmented Generation (RAG)\n2020\nCombines information retrieval with text generation.\nFocuses on enhancing generation with retrieved data.\nChatbots, question answering systems\n\n\nGenerative Adversarial Networks (GANs)\n2014\nConsists of a generator and a discriminator competing against each other.\nUses adversarial training for realistic data generation.\nImage and video generation\n\n\nLarge Language Models (LLMs)\n2018\nDeep learning models trained on vast amounts of text data.\nSpecializes in understanding and generating human-like text.\nText generation, summarization, translation\n\n\nGPT (Generative Pre-trained Transformer)\n2018\nA specific type of LLM developed by OpenAI using transformer architecture.\nPre-trained on large datasets for coherent text generation.\nConversational agents, content creation\n\n\nAutoencoders\n1987\nNeural networks for learning efficient data representations.\nComposed of an encoder and decoder for data compression.\nDenoising, anomaly detection\n\n\nTransformers\n2017\nNeural network architecture using self-attention mechanisms.\nDesigned for processing sequential data effectively.\nNatural language processing, image analysis\n\n\n\n\n\n\n\nComparison of three categories of generative models.\n\n\n\n\nVariational Autoencoders (VAEs)\n\nVariational AutoEncoders (VAEs) is a type of autoencoder that extends the basic architecture to learn a probabilistic model of the data.\n\nThis allows them to generate new data similar to the original input but not identical.\nThe key innovation in VAEs is the introduction of a regularization term known as the Kullback-Leibler (KL) divergence, which encourages the learned distribution to match a prior distribution, typically a standard normal distribution.\nThis regularization term allows VAEs to generate more diverse and realistic data than traditional autoencoders.\n\n\n\nIllustration of variational autoencoder model\n\n\n\nFrom Autoencoder to Beta-VAE\n\n\n\nGenerative Adversarial Networks (GANs)\nGenerative Adversarial Networks (GANs) were introduced by Ian Goodfellow in 2014. They are a class of generative models that use two neural networks, a generator, and a discriminator, to generate new data.\n\nThe generator network takes a random noise vector as input and generates a sample from the data distribution. The discriminator network takes a sample from the data distribution and a sample from the generator network and tries to distinguish between them.\n\nThe generator network is trained to fool the discriminator network, while the discriminator network is trained to distinguish between real and fake samples.\n\n\n\nIllustration of GAN model\n\n\n\nFrom GAN to WGAN\n\n\n\nTransformers\nThe Transformer was introduced in 2017 by Vaswani et al.\n\nIt is a neural network architecture that uses attention mechanisms to process data sequences.\n\nThe Transformer has been used in many applications, such as machine translation, text summarization, and image captioning.\n\n\n\nTransformer model architecture\n\n\n\nThe Transformer Blueprint: A Holistic Guide to the Transformer Neural Network Architecture\n\n\n\nLarge Language Models (LLM)\n\nLLMs are trained on vast amounts of textual data and use transformer models to analyze and produce coherent, contextually relevant language.\n\nThey excel at tasks such as text generation, translation, and answering questions based on their training data.\n\n\nResources\n\nAn overview of Generative AI in 2023\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Generative AI",
      "Generative AI"
    ]
  },
  {
    "objectID": "generative/generative-rag.html",
    "href": "generative/generative-rag.html",
    "title": "Generative AI RAG",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n\n\n Back to top",
    "crumbs": [
      "Generative RAG",
      "Generative AI RAG"
    ]
  },
  {
    "objectID": "generative/generative-llm.html",
    "href": "generative/generative-llm.html",
    "title": "Generative LLM",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n\n\n Back to top",
    "crumbs": [
      "Generative LLM",
      "Generative LLM"
    ]
  }
]