---
title: "Labs"
---


# Lab#ANN001: Buiding ANN recognises numbers

https://youtu.be/w8yWXqWQYmU?feature=shared

Kaggle notebook with all the code: https://www.kaggle.com/wwsalmon/...

Blog article with more/clearer math explanation: https://www.samsonzhang.com/2020...

# Lab#GPT: Pyrhon extractor

AI reads books: Page-by-Page PDF Knowledge Extractor & Summarizer

The read_books.py script performs an intelligent page-by-page analysis of PDF books, methodically extracting knowledge points and generating progressive summaries at specified intervals. It processes each page individually, allowing for detailed content understanding while maintaining the contextual flow of the book. Below is a detailed explanation of how the script works: 

https://github.com/echohive42/AI-reads-books-page-by-pag

# Lab#ANN: Tensor for back propagation

A small auto-grad engine (inspired from Karpathy's micrograd and PyTorch) using Apple's MLX and Numpy. That's why the name: smolgrad

it will help y'all understand the core concepts behind automatic differentiation and backpropagation. I mean, AI is literally powered by these things.

what is autograd?

auto-grad, short for automatic differentiation, is a technique used in machine learning to efficiently compute gradients of complex functions. In the context of neural networks, autograd enables the computation of gradients with respect to the model's parameters, which is crucial for training the network using optimization algorithms like gradient descent.

it works by constructing a computational graph that represents the flow of data through the network. Each node in the graph represents a mathematical operation, and the edges represent the flow of data between these operations. By tracking the operations and their dependencies, autograd can automatically compute the gradients using the chain rule of calculus.

an algorithm used in conjuction with autograd is Backpropagation to train neural networks. It is the process of propagating the gradients backward through the computational graph, from the output layer to the input layer, in order to update the model's parameters. This is the stuff which makes ML/DL models "learn".

https://github.com/smolorg/smolgrad