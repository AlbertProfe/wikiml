---
title: "Labs"
---


# Lab#ANN001: Building ANN recognizes numbers

- [Building a neural network FROM SCRATCH (no Tensorflow/Pytorch, just numpy & math)](https://youtu.be/w8yWXqWQYmU?feature=shared){.external target='_blank'}
- [Kaggle notebook with all the code](https://www.kaggle.com/code/wwsalmon/simple-mnist-nn-from-scratch-numpy-no-tf-keras){.external target='_blank'}
- [Blog article with more/clearer math explanation](https://www.samsonzhang.com/building){.external target='_blank'}




# Lab#GPT001: Python extractor

**AI reads books**: Page-by-Page PDF Knowledge Extractor & Summarizer

> The `read_books.py` script performs an intelligent page-by-page analysis of PDF books, methodically extracting knowledge points and generating progressive summaries at specified intervals. It processes each page individually, allowing for detailed content understanding while maintaining the contextual flow of the book.

- [Python extractor](https://github.com/echohive42/AI-reads-books-page-by-pag){.external target='_blank'}

# Lab#ANN002: Tensor for back propagation

A small auto-grad engine (inspired from `Karpathy's micrograd and PyTorch`) using `Apple's MLX and Numpy`. That's why the name: smolgrad

> It will help y'all understand the core concepts behind automatic differentiation and backpropagation. I mean, AI is literally powered by these things.

**what is autograd?**

Auto-grad, short for automatic differentiation, is a technique used in machine learning to efficiently compute gradients of complex functions. In the context of neural networks, autograd enables the computation of gradients with respect to the model's parameters, which is crucial for training the network using optimization algorithms like gradient descent.

It works by constructing a computational graph that represents the flow of data through the network. Each node in the graph represents a mathematical operation, and the edges represent the flow of data between these operations. By tracking the operations and their dependencies, autograd can automatically compute the gradients using the chain rule of calculus.

An algorithm used in conjuction with autograd is Backpropagation to train neural networks. It is the process of propagating the gradients backward through the computational graph, from the output layer to the input layer, in order to update the model's parameters. This is the stuff which makes ML/DL models "learn".

- [utograd: repo](https://github.com/smolorg/smolgrad){.external target='_blank'}